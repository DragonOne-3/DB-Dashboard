import streamlit as st
import pandas as pd
from datetime import datetime
import io
import json
import requests
from google.oauth2 import service_account
import google.auth.transport.requests
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload

# --- 1. í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="ê³µê³µì¡°ë‹¬ DATA í†µí•©ê²€ìƒ‰", layout="wide")

# --- 2. êµ¬ê¸€ ì¸ì¦ ì„¤ì • (ì‚¬ìš©ìë‹˜ ì œê³µ GOOGLE_AUTH_JSON ë°©ì‹) ---
@st.cache_resource
def get_drive_service():
    try:
        # ì‹œí¬ë¦¿ì—ì„œ GOOGLE_AUTH_JSON ë¬¸ìì—´ì„ ê°€ì ¸ì™€ JSONìœ¼ë¡œ íŒŒì‹±
        auth_json_str = st.secrets["GOOGLE_AUTH_JSON"]
        info = json.loads(auth_json_str)
        
        creds = service_account.Credentials.from_service_account_info(
            info, 
            scopes=['https://www.googleapis.com/auth/drive.readonly', 
                    'https://www.googleapis.com/auth/spreadsheets.readonly']
        )
        return build('drive', 'v3', credentials=creds), creds
    except Exception as e:
        st.error(f"ì¸ì¦ ì´ˆê¸°í™” ì‹¤íŒ¨: ì‹œí¬ë¦¿ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”. ({e})")
        st.stop()

drive_service, credentials = get_drive_service()

# --- 3. ë°ì´í„° ì†ŒìŠ¤ ì •ë³´ ---
CSV_FOLDER_ID = '1N2GjNTpOvtn-5Vbg5zf6Y8kf4xuq0qTr' 
SHEET_FILE_IDS = {
    'ë‚˜ë¼ì¥í„°_ë°œì£¼': '1pGnb6O5Z1ahaHYuQdydyoY1Ayf147IoGmLRdA3WAHi4',
    'ë‚˜ë¼ì¥í„°_ê³„ì•½': '15Hsr_nup4ZteIZ4Jyov8wG2s_rKoZ25muqRE3-sRnaw',
    'êµ°ìˆ˜í’ˆ_ë°œì£¼': '1pzW51Z29SSoQk7al_GvN_tj5smuhOR3J2HWnL_16fcI',
    'êµ°ìˆ˜í’ˆ_ê³„ì•½': '1KPMUz0IKM6AQvqwfAkvW96WNvzbycN56vNlFnDmfRTw',
    'êµ°ìˆ˜í’ˆ_ê³µê³ ': '1opuA_UzNm27U9QkbMay5UsyQqcwfxiEmIHNRdc4MyHM',
    'êµ°ìˆ˜í’ˆ_ìˆ˜ì˜': '1aYA18kPrSkpbayzbn16EdKUScVRwr2Nutyid5No5qjk',
    'ì¢…í•©ì‡¼í•‘ëª°': 'FOLDER'
}
DATE_COL_MAP = {
    'êµ°ìˆ˜í’ˆ_ë°œì£¼': 'ë°œì£¼ì˜ˆì •ì›”', 'êµ°ìˆ˜í’ˆ_ìˆ˜ì˜': 'ê°œì°°ì¼ì', 'êµ°ìˆ˜í’ˆ_ê³„ì•½': 'ê³„ì•½ì¼ì',
    'êµ°ìˆ˜í’ˆ_ê³µê³ ': 'ê³µê³ ì¼ì', 'ë‚˜ë¼ì¥í„°_ê³„ì•½': 'â˜…ê°€ê³µ_ê³„ì•½ì¼', 'ë‚˜ë¼ì¥í„°_ë°œì£¼': 'ê³µê³ ì¼ì', 'ì¢…í•©ì‡¼í•‘ëª°': 'ê³„ì•½ë‚©í’ˆìš”êµ¬ì¼ì'
}

# --- 4. ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ---
@st.cache_data(ttl=3600, show_spinner=False)
def load_large_sheet(file_id):
    auth_req = google.auth.transport.requests.Request()
    credentials.refresh(auth_req)
    url = f"https://docs.google.com/spreadsheets/d/{file_id}/export?format=csv"
    headers = {'Authorization': f'Bearer {credentials.token}'}
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return pd.read_csv(io.BytesIO(response.content), low_memory=False)
    return pd.DataFrame()

@st.cache_data(ttl=3600, show_spinner=False)
def load_csv_file(file_id):
    request = drive_service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        _, done = downloader.next_chunk()
    fh.seek(0)
    return pd.read_csv(fh, low_memory=False)

# --- 5. ì‚¬ì´ë“œë°” ê²€ìƒ‰ UI ---
with st.sidebar:
    st.header("ğŸ” ê²€ìƒ‰ ì¡°ê±´ ì„¤ì •")
    
    # ì§€ìì²´ ìœ ì§€ë³´ìˆ˜ ì‚¬ì´íŠ¸ ì´ë™ ë‹¨ì¶” ì¶”ê°€
    st.link_button("ğŸŒ ì§€ìì²´ ìœ ì§€ë³´ìˆ˜ ì‚¬ì´íŠ¸ ì´ë™", "https://www.g2b.go.kr/", use_container_width=True)
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    start_date = col1.date_input("ì¡°íšŒ ì‹œì‘ì¼", datetime(2025, 1, 1))
    end_date = col2.date_input("ì¡°íšŒ ì¢…ë£Œì¼", datetime.now())
    
    search_field = st.selectbox("ê²€ìƒ‰ í•„ë“œ", ["ALL", "ìˆ˜ìš”ê¸°ê´€ëª…", "ì—…ì²´ëª…", "ê³„ì•½ëª…", "ì„¸ë¶€í’ˆëª…"])
    k1 = st.text_input("ì²« ë²ˆì§¸ ê²€ìƒ‰ì–´")
    logic = st.selectbox("ê²€ìƒ‰ ë…¼ë¦¬", ["NONE", "AND", "OR"])
    k2 = st.text_input("ë‘ ë²ˆì§¸ ê²€ìƒ‰ì–´") if logic != "NONE" else ""
    
    search_btn = st.button("ğŸš€ ë°ì´í„° ì¡°íšŒ ì‹¤í–‰", type="primary", use_container_width=True)

# --- 6. ë©”ì¸ í™”ë©´: íƒ­ êµ¬ì„± ---
st.title("ğŸ› ê³µê³µì¡°ë‹¬ DATA í†µí•©ê²€ìƒ‰")

# ë“œë¡­ë°•ìŠ¤ ëŒ€ì‹  íƒ­(Tabs) ì‚¬ìš©
tabs = st.tabs(list(SHEET_FILE_IDS.keys()))

for i, tab in enumerate(tabs):
    category_name = list(SHEET_FILE_IDS.keys())[i]
    
    with tab:
        if search_btn:
            with st.spinner(f"{category_name} ë°ì´í„° ë¶„ì„ ì¤‘..."):
                try:
                    df = pd.DataFrame()
                    s_str = start_date.strftime('%Y%m%d')
                    e_str = end_date.strftime('%Y%m%d')

                    # ë°ì´í„° ë¡œë“œ ë¡œì§
                    if category_name == 'ì¢…í•©ì‡¼í•‘ëª°':
                        results = drive_service.files().list(q=f"'{CSV_FOLDER_ID}' in parents and trashed = false").execute()
                        files = results.get('files', [])
                        relevant_dfs = []
                        for f in files:
                            if any(str(y) in f['name'] for y in range(start_date.year, end_date.year + 1)):
                                tmp = load_csv_file(f['id'])
                                if not tmp.empty:
                                    date_col = tmp.columns[3]
                                    tmp['compare_date'] = tmp[date_col].astype(str).str.replace(r'[^0-9]', '', regex=True).str[:8]
                                    relevant_dfs.append(tmp[(tmp['compare_date'] >= s_str) & (tmp['compare_date'] <= e_str)])
                        if relevant_dfs: df = pd.concat(relevant_dfs, ignore_index=True)
                    else:
                        df = load_large_sheet(SHEET_FILE_IDS[category_name])
                        date_col_name = DATE_COL_MAP.get(category_name)
                        if not df.empty and date_col_name in df.columns:
                            df['compare_date'] = df[date_col_name].astype(str).str.replace(r'[^0-9]', '', regex=True).str[:8]
                            df = df[(df['compare_date'] >= s_str) & (df['compare_date'] <= e_str)]

                    # í‚¤ì›Œë“œ í•„í„°ë§
                    if not df.empty and k1:
                        mask = df.astype(str).apply(lambda x: x.str.contains(k1, case=False, na=False)).any(axis=1) if search_field=="ALL" else df[search_field].astype(str).str.contains(k1, case=False, na=False)
                        if logic == "AND" and k2:
                            mask2 = df.astype(str).apply(lambda x: x.str.contains(k2, case=False, na=False)).any(axis=1) if search_field=="ALL" else df[search_field].astype(str).str.contains(k2, case=False, na=False)
                            df = df[mask & mask2]
                        elif logic == "OR" and k2:
                            mask2 = df.astype(str).apply(lambda x: x.str.contains(k2, case=False, na=False)).any(axis=1) if search_field=="ALL" else df[search_field].astype(str).str.contains(k2, case=False, na=False)
                            df = df[mask | mask2]
                        else:
                            df = df[mask]

                    # ê²°ê³¼ í‘œì‹œ
                    if not df.empty:
                        st.success(f"âœ… {category_name}: {len(df):,}ê±´ ì¡°íšŒ ì™„ë£Œ")
                        st.dataframe(df.drop(columns=['compare_date'], errors='ignore'), use_container_width=True, height=500)
                        csv = df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                        st.download_button(f"ğŸ“¥ {category_name} ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", csv, f"{category_name}.csv", "text/csv")
                    else:
                        st.info(f"ì¡°íšŒ ê¸°ê°„ ë‚´ì— {category_name} ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            st.write("ì‚¬ì´ë“œë°”ì—ì„œ ì¡°ê±´ì„ ì…ë ¥í•˜ê³  **ë°ì´í„° ì¡°íšŒ ì‹¤í–‰** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

st.caption("ğŸ› ê³µê³µì¡°ë‹¬ DATA í†µí•©ê²€ìƒ‰ ì‹œìŠ¤í…œ | ì‚¬ìš©ì ì§€ì • ì¸ì¦ ë³´ì•ˆ ëª¨ë“œ")
