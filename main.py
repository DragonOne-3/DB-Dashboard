import os, json, datetime, time, requests
import xml.etree.ElementTree as ET

# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë° ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ëª¨ë“ˆ
import pandas as pd
import io
import threading
from googleapiclient.discovery import build
from google.oauth2 import service_account
from googleapiclient.http import MediaIoBaseUpload 

# ê¸°íƒ€ ìœ í‹¸ë¦¬í‹°
from pytimekr import pytimekr # ê³µíœ´ì¼ ê³„ì‚°
import re

# ================= 1. ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜ =================
MY_DIRECT_KEY = os.environ.get('DATA_GO_KR_API_KEY')
AUTH_JSON_STR = os.environ.get('GOOGLE_AUTH_JSON')
save_lock = threading.Lock() # íŒŒì¼ ì“°ê¸° ì¶©ëŒ ë°©ì§€ìš© ì ê¸ˆ ì¥ì¹˜

# [ê¸°ì¡´ ìœ ì§€] ë¬¼í’ˆ ë‚©í’ˆ ìƒì„¸ ë‚´ì—­ìš© í—¤ë” (43ê°œ ì „ì²´)
HEADER_KOR = ['ì¡°ë‹¬êµ¬ë¶„ëª…', 'ê³„ì•½êµ¬ë¶„ëª…', 'ê³„ì•½ë‚©í’ˆêµ¬ë¶„ëª…', 'ê³„ì•½ë‚©í’ˆìš”êµ¬ì¼ì', 'ê³„ì•½ë‚©í’ˆìš”êµ¬ë²ˆí˜¸', 'ë³€ê²½ì°¨ìˆ˜', 'ìµœì¢…ë³€ê²½ì°¨ìˆ˜ì—¬ë¶€', 'ìˆ˜ìš”ê¸°ê´€ëª…', 'ìˆ˜ìš”ê¸°ê´€êµ¬ë¶„ëª…', 'ìˆ˜ìš”ê¸°ê´€ì§€ì—­ëª…', 'ìˆ˜ìš”ê¸°ê´€ì½”ë“œ', 'ë¬¼í’ˆë¶„ë¥˜ë²ˆí˜¸', 'í’ˆëª…', 'ì„¸ë¶€ë¬¼í’ˆë¶„ë¥˜ë²ˆí˜¸', 'ì„¸ë¶€í’ˆëª…', 'ë¬¼í’ˆì‹ë³„ë²ˆí˜¸', 'ë¬¼í’ˆê·œê²©ëª…', 'ë‹¨ê°€', 'ìˆ˜ëŸ‰', 'ë‹¨ìœ„', 'ê¸ˆì•¡', 'ì—…ì²´ëª…', 'ì—…ì²´ê¸°ì—…êµ¬ë¶„ëª…', 'ê³„ì•½ëª…', 'ìš°ìˆ˜ì œí’ˆì—¬ë¶€', 'ê³µì‚¬ìš©ìì¬ì§ì ‘êµ¬ë§¤ëŒ€ìƒì—¬ë¶€', 'ë‹¤ìˆ˜ê³µê¸‰ìê³„ì•½ì—¬ë¶€', 'ë‹¤ìˆ˜ê³µê¸‰ìê³„ì•½2ë‹¨ê³„ì§„í–‰ì—¬ë¶€', 'ë‹¨ê°€ê³„ì•½ë²ˆí˜¸', 'ë‹¨ê°€ê³„ì•½ë³€ê²½ì°¨ìˆ˜', 'ìµœì´ˆê³„ì•½(ë‚©í’ˆìš”êµ¬)ì¼ì', 'ê³„ì•½ì²´ê²°ë°©ë²•ëª…', 'ì¦ê°ìˆ˜ëŸ‰', 'ì¦ê°ê¸ˆì•¡', 'ë‚©í’ˆì¥ì†Œëª…', 'ë‚©í’ˆê¸°í•œì¼ì', 'ì—…ì²´ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸', 'ì¸ë„ì¡°ê±´ëª…', 'ë¬¼í’ˆìˆœë²ˆ']

# ğŸš€ [ì¶”ê°€/ìˆ˜ì •] ì£¼ì°¨ê´€ì œ ë° ì¶œì…í†µì œ í’ˆëª© í†µí•© í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ì œê±° ë° ì •ë ¬ ì™„ë£Œ)
keywords = sorted(list(set([
    'ë„¤íŠ¸ì›Œí¬ì‹œìŠ¤í…œì¥ë¹„ìš©ë™','ì˜ìƒê°ì‹œì¥ì¹˜','PAìš©ìŠ¤í”¼ì»¤','ì•ˆë‚´íŒ','ì¹´ë©”ë¼ë¸Œë˜í‚·','ì•¡ì •ëª¨ë‹ˆí„°','ê´‘ì†¡ìˆ˜ì‹ ëª¨ë“ˆ','ì „ì›ê³µê¸‰ì¥ì¹˜','ê´‘ë¶„ë°°í•¨','ì»¨ë²„í„°','ì»´í“¨í„°ì„œë²„','í•˜ë“œë””ìŠ¤í¬ë“œë¼ì´ë¸Œ','ë„¤íŠ¸ì›Œí¬ìŠ¤ìœ„ì¹˜','ê´‘ì í¼ì½”ë“œ','í’€ë°•ìŠ¤','ì„œì§€í¡ìˆ˜ê¸°','ë””ì§€í„¸ë¹„ë””ì˜¤ë ˆì½”ë”',
    'ìŠ¤í”¼ì»¤','ì˜¤ë””ì˜¤ì•°í”„','ë¸Œë˜í‚·','UTPì¼€ì´ë¸”','ì •ë³´í†µì‹ ê³µì‚¬','ì˜ìƒì •ë³´ë””ìŠ¤í”Œë ˆì´ì¥ì¹˜','ì†¡ì‹ ê¸°','ë‚œì—°ì „ë ¥ì¼€ì´ë¸”','1ì¢…ê¸ˆì†ì œê°€ìš”ì „ì„ ê´€','í˜¸ì˜¨ìŠ¤í”¼ì»¤','ëˆ„ì „ì°¨ë‹¨ê¸°','ë°©ì†¡ìˆ˜ì‹ ê¸°','LAPì™¸í”¼ê´‘ì¼€ì´ë¸”','í´ë¦¬ì—í‹¸ë Œì „ì„ ê´€','ë¦¬ëª¨íŠ¸ì•°í”„',
    'ë™ìºë¹„ë‹›ìš©íŒ¨ë„','ë² ì–´ë³¸ì»´í“¨í„°','ë¶„ë°°ê¸°','ê²°ì„ ë³´ë“œìœ ë‹›','ë²¨','ë‚œì—°ì ‘ì§€ìš©ë¹„ë‹ì ˆì—°ì „ì„ ','ê²½ê´‘ë“±','ë°ìŠ¤í¬í†±ì»´í“¨í„°','íŠ¹ìˆ˜ëª©ì ì»´í“¨í„°','ì² ê·¼ì½˜í¬ë¦¬íŠ¸ê³µì‚¬','í† ê³µì‚¬','ì•ˆë‚´ì „ê´‘íŒ','ì ‘ì§€ë´‰','ì¹´ë©”ë¼íšŒì „ëŒ€','ë¬´ì„ ëœì•¡ì„¸ìŠ¤í¬ì¸íŠ¸','ì»´í“¨í„°ë§ì „í™˜ì¥ì¹˜',
    'í¬ì¥ê³µì‚¬','ê³ ì£¼íŒŒë™ì¶•ì¼€ì´ë¸”','ì¹´ë©”ë¼í•˜ìš°ì§•','ì¸í„°í°','ìŠ¤ìœ„ì¹­ëª¨ë“œì „ì›ê³µê¸‰ì¥ì¹˜','ê¸ˆì†ìƒì','ì—´ì„ ê°ì§€ê¸°','íƒœì–‘ì „ì§€ì¡°ì ˆê¸°','ë°€íê³ ì •í˜•ë‚©ì¶•ì „ì§€','IPì „í™”ê¸°','ë””ìŠ¤í¬ì–´ë ˆì´','ê·¸ë˜í”½ìš©ì–´ëŒ‘í„°','ì¸í„°ì½¤ì¥ë¹„','ê¸°ì–µìœ ë‹›','ì»´í“¨í„°ì§€ë¬¸ì¸ì‹ì¥ì¹˜','ëœì ‘ì†ì¹´ë“œ',
    'ì ‘ì§€íŒ','ì œì–´ì¼€ì´ë¸”','ë¹„ë””ì˜¤ë„¤íŠ¸ì›Œí‚¹ì¥ë¹„','ë ˆì´ìŠ¤ì›¨ì´','ì½˜ì†”ìµìŠ¤í…ë”','ì „ìì¹´ë“œ','ë¹„ëŒ€ë©´ë°©ì—­ê°ì§€ì¥ë¹„','ì˜¨ìŠµë„íŠ¸ëœìŠ¤ë¯¸í„°','ë„ë‚œë°©ì§€ê¸°','ìœµë³µí•©ì˜ìƒê°ì‹œì¥ì¹˜','ë©€í‹°ìŠ¤í¬ë¦°ì»´í“¨í„°','ì»´í“¨í„°ì •ë§¥ì¸ì‹ì¥ì¹˜','ì¹´ë©”ë¼ì»¨íŠ¸ë¡¤ëŸ¬','SSDì €ì¥ì¥ì¹˜','ì›ê²©ë‹¨ë§ì¥ì¹˜(RTU)',
    'ìœµë³µí•©ë„¤íŠ¸ì›Œí¬ìŠ¤ìœ„ì¹˜','ìœµë³µí•©ì•¡ì •ëª¨ë‹ˆí„°','ìœµë³µí•©ë°ìŠ¤í¬í†±ì»´í“¨í„°','ìœµë³µí•©ê·¸ë˜í”½ìš©ì–´ëŒ‘í„°','ìœµë³µí•©ë² ì–´ë³¸ì»´í“¨í„°','ìœµë³µí•©ì„œì§€í¡ìˆ˜ê¸°','ë°°ì„ ì¥ì¹˜','ìœµë³µí•©ë°°ì„ ì¥ì¹˜','ìœµë³µí•©ì¹´ë©”ë¼ë¸Œë˜í‚·','ìœµë³µí•©ë„¤íŠ¸ì›Œí¬ì‹œìŠ¤í…œì¥ë¹„ìš©ë™','ìœµë³µí•©UTPì¼€ì´ë¸”','í…Œì´í”„ë°±ì—…ì¥ì¹˜',
    'ìê¸°ì‹í…Œì´í”„','ë ˆì´ë“œì €ì¥ì¥ì¹˜','ê´‘ì†¡ìˆ˜ì‹ ê¸°','450/750V ìœ ì—°ì„±ë‹¨ì‹¬ë¹„ë‹ì ˆì—°ì „ì„ ','ì†”ë‚´ì‹œìŠ¤í…œ','450/750Vìœ ì—°ì„±ë‹¨ì‹¬ë¹„ë‹ì ˆì—°ì „ì„ ','ì¹´ë©”ë¼ë°›ì¹¨ëŒ€','í…”ë ˆë¹„ì „ê±°ì¹˜ëŒ€','ê´‘ìˆ˜ì‹ ê¸°','ë¬´ì„ í†µì‹ ì¥ì¹˜','ë™ì‘ë¶„ì„ê¸°','ì „ë ¥ê³µê¸‰ì¥ì¹˜','450/750V ì¼ë°˜ìš©ìœ ì—°ì„±ë‹¨ì‹¬ë¹„ë‹ì ˆì—°ì „ì„ ','ë¶„ì „í•¨',
    'ë¹„ë””ì˜¤ë¯¹ì„œ','ì ˆì—°ì „ì„ ë°í”¼ë³µì„ ','ë ˆì´ë”','ì ì™¸ì„ ë°©ì‚¬ê¸°', 'ë³´ì•ˆìš©ì¹´ë©”ë¼', 'í†µì‹ ì†Œí”„íŠ¸ì›¨ì–´','ë¶„ì„ë°ê³¼í•™ìš©ì†Œí”„íŠ¸ì›¨ì–´','ì†Œí”„íŠ¸ì›¨ì–´ìœ ì§€ë°ì§€ì›ì„œë¹„ìŠ¤',
    # --- ì£¼ì°¨ê´€ì œ ë° ì¶œì…í†µì œ ì¶”ê°€ í’ˆëª© ---
    'êµí†µê´€ì œì‹œìŠ¤í…œ', 'ë¶„ì„ë°ê³¼í•™ìš©ì†Œí”„íŠ¸ì›¨ì–´', 'ì‚°ì—…ê´€ë¦¬ì†Œí”„íŠ¸ì›¨ì–´', 'ì‹œìŠ¤í…œê´€ë¦¬ì†Œí”„íŠ¸ì›¨ì–´', 'ì˜¨ìŠµë„íŠ¸ëœìŠ¤ë¯¸í„°', 'ì¸í„°ì½¤ì¥ë¹„', 'ìê¸°ì‹í…Œì´í”„', 'ì ì™¸ì„ ì¹´ë©”ë¼', 
    'ì£¼ì°¨ê²½ë³´ë“±', 'ì£¼ì°¨ê´€ì œì£¼ë³€ê¸°ê¸°', 'ì£¼ì°¨ê¶ŒíŒë…ê¸°', 'ì£¼ì°¨ì•ˆë‚´íŒ', 'ì£¼ì°¨ìš”ê¸ˆê³„ì‚°ê¸°', 'ì£¼ì°¨ì£¼ì œì–´ì¥ì¹˜', 'ì°¨ëŸ‰ê°ì§€ê¸°', 'ì°¨ëŸ‰ì¸ì‹ê¸°', 'ì°¨ëŸ‰ì°¨ë‹¨ê¸°', 
    'íŒ¨í‚¤ì§€ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œë°ë„ì…ì„œë¹„ìŠ¤', 'ë„ë‚œë°©ì§€ê¸°', 'ë¬´ì„ ì¸ì‹ë¦¬ë”ê¸°', 'ë°”ì½”ë“œì‹œìŠ¤í…œ', 'ë¹„ëŒ€ë©´ë°©ì—­ê°ì§€ì¥ë¹„', 'ì¶œì…í†µì œì‹œìŠ¤í…œ', 'ì¹´ë“œì¸ì‡„ê¸°', 'ì»´í“¨í„°ì •ë§¥ì¸ì‹ì¥ì¹˜'
])))

# [ì¶”ê°€] ì…ì°° ê³µê³  API ì£¼ì†Œ
NOTICE_API_MAP = {
    'ê³µì‚¬': 'https://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoCnstwkPPSSrch',
    'ë¬¼í’ˆ': 'https://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoThngPPSSrch',
    'ìš©ì—­': 'https://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoServcPPSSrch'
}

# ================= 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =================
def get_drive_service_for_script():
    info = json.loads(AUTH_JSON_STR)
    creds = service_account.Credentials.from_service_account_info(info, scopes=['https://www.googleapis.com/auth/drive'])
    return build('drive', 'v3', credentials=creds), creds

def get_target_date():
    now = datetime.datetime.utcnow() + datetime.timedelta(hours=9)
    target = now - datetime.timedelta(days=1)
    holidays = pytimekr.holidays(year=target.year)
    while target.weekday() >= 5 or target.date() in holidays:
        target -= datetime.timedelta(days=1)
    return target

def fetch_api_data_from_g2b(kw, d_str):
    url = "https://apis.data.go.kr/1230000/at/ShoppingMallPrdctInfoService/getSpcifyPrdlstPrcureInfoList"
    params = {'numOfRows': '999', 'pageNo': '1', 'ServiceKey': MY_DIRECT_KEY, 'Type_A': 'xml', 'inqryDiv': '1', 'inqryPrdctDiv': '2', 'inqryBgnDate': d_str, 'inqryEndDate': d_str, 'dtilPrdctClsfcNoNm': kw}
    try:
        res = requests.get(url, params=params, timeout=30)
        if res.status_code == 200 and "<item>" in res.text:
            root = ET.fromstring(res.content)
            return [[elem.text if elem.text else '' for elem in elem_item] for elem_item in root.findall('.//item')]
    except: pass
    return []

def fetch_and_generate_servc_html(target_dt):
    api_key = os.environ.get('DATA_GO_KR_API_KEY')
    api_url = 'http://apis.data.go.kr/1230000/ao/CntrctInfoService/getCntrctInfoListServcPPSSrch'
    target_date_str = target_dt.strftime("%Y%m%d")
    display_date_str = target_dt.strftime("%Y-%m-%d")
    keywords_servc = ['í†µí•©ê´€ì œ', 'CCTV', 'ì˜ìƒê°ì‹œì¥ì¹˜','êµ­ë°©','ê²½ê³„','ì‘ì „','ë¶€ëŒ€','ìœ¡êµ°','ê³µêµ°','í•´êµ°','ë¬´ì¸']
    collected_data = []

    for kw in keywords_servc:
        params = {'serviceKey': api_key, 'pageNo': '1', 'numOfRows': '999', 'inqryDiv': '1', 'type': 'xml', 'inqryBgnDate': target_date_str, 'inqryEndDate': target_date_str, 'cntrctNm': kw}
        try:
            res = requests.get(api_url, params=params, timeout=30)
            if res.status_code == 200:
                root = ET.fromstring(res.content)
                for item in root.findall('.//item'):
                    raw_demand = item.findtext('dminsttList', '-')
                    raw_corp = item.findtext('corpList', '-')
                    demand_parts = raw_demand.replace('[', '').replace(']', '').split('^')
                    clean_demand = demand_parts[2] if len(demand_parts) > 2 else raw_demand
                    corp_parts = raw_corp.replace('[', '').replace(']', '').split('^')
                    clean_corp = corp_parts[3] if len(corp_parts) > 3 else raw_corp
                    collected_data.append({
                        'demand': clean_demand, 'name': item.findtext('cntrctNm', '-'), 'corp': clean_corp,
                        'amount': int(item.findtext('totCntrctAmt', '0')), 'date': display_date_str
                    })
        except: pass

    unique_servc = {f"{d['demand']}_{d['name']}": d for d in collected_data}.values()
    html = f"<div style='margin-top: 20px;'><h4 style='color: #1a73e8; border-bottom: 2px solid #1a73e8; padding-bottom: 5px;'>ğŸ›ï¸ ë‚˜ë¼ì¥í„° ìš©ì—­ ê³„ì•½ ë‚´ì—­ ({display_date_str})</h4>"
    if not unique_servc:
        html += f"<p style='color: #666;'>- {display_date_str}ì— í•´ë‹¹ í‚¤ì›Œë“œ ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.</p></div>"
        return html

    html += "<table border='1' style='border-collapse: collapse; width: 100%; font-size: 11px;'> <tr style='background-color: #f8f9fa;'><th>ìˆ˜ìš”ê¸°ê´€</th><th>ê³„ì•½ëª…</th><th>ì—…ì²´ëª…</th><th>ê¸ˆì•¡</th></tr>"
    for row in unique_servc:
        bg = "background-color: #FFF9C4;" if "ì´ë…¸ë" in row['corp'] else ""
        html += f"<tr style='{bg}'><td>{row['demand']}</td><td>{row['name']}</td><td>{row['corp']}</td><td style='text-align: right;'>{row['amount']:,}ì›</td></tr>"
    html += "</table></div>"
    return html

def fetch_notice_data(category, url, d_str):
    params = {'serviceKey': MY_DIRECT_KEY, 'pageNo': '1', 'numOfRows': '999', 'inqryDiv': '1', 'type': 'json', 'inqryBgnDt': d_str + "0000", 'inqryEndDt': d_str + "2359"}
    try:
        res = requests.get(url, params=params, timeout=45)
        if res.status_code == 200:
            items = res.json().get('response', {}).get('body', {}).get('items', [])
            return pd.DataFrame(items)
    except: pass
    return pd.DataFrame()

# ================= 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§ =================
def main():
    if not MY_DIRECT_KEY or not AUTH_JSON_STR: return

    target_dt = get_target_date()
    d_str = target_dt.strftime("%Y%m%d")
    drive_service, drive_creds = get_drive_service_for_script()
    
    # --- PART 1: ë¬¼í’ˆ ë‚©í’ˆ ìƒì„¸ ë‚´ì—­ ìˆ˜ì§‘ (ê¸°ì¡´ ê¸°ëŠ¥) ---
    final_data = []
    for kw in keywords:
        data = fetch_api_data_from_g2b(kw, d_str)
        if data: final_data.extend(data)
        time.sleep(0.5)
    
    if final_data:
        new_df = pd.DataFrame(final_data, columns=HEADER_KOR)
        DRIVE_FOLDER_ID = '1N2GjNTpOvtn-5Vbg5zf6Y8kf4xuq0qTr'
        FILE_NAME_FOR_YEAR = f"{target_dt.year}.csv"

        # ë“œë¼ì´ë¸Œ íŒŒì¼ ê²€ìƒ‰ ë° ì—…ë°ì´íŠ¸
        query = f"name='{FILE_NAME_FOR_YEAR}' and '{DRIVE_FOLDER_ID}' in parents and trashed=false and mimeType='text/csv'" 
        res = drive_service.files().list(q=query, spaces='drive', fields='files(id)').execute()
        items = res.get('files', [])

        if items:
            f_id = items[0]['id']
            d_url = f'https://www.googleapis.com/drive/v3/files/{f_id}?alt=media'
            resp = requests.get(d_url, headers={'Authorization': f'Bearer {drive_creds.token}'})
            if resp.status_code == 200:
                old_df = pd.read_csv(io.BytesIO(resp.content), encoding='utf-8-sig', low_memory=False)
                df_to_upload = pd.concat([old_df, new_df], ignore_index=True).drop_duplicates(subset=['ê³„ì•½ë‚©í’ˆìš”êµ¬ì¼ì', 'ìˆ˜ìš”ê¸°ê´€ëª…', 'í’ˆëª…', 'ê¸ˆì•¡'], keep='last')
            else: df_to_upload = new_df
        else:
            f_id = None
            df_to_upload = new_df

        csv_bytes = df_to_upload.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
        media = MediaIoBaseUpload(io.BytesIO(csv_bytes), mimetype='text/csv', resumable=True)

        if f_id: drive_service.files().update(fileId=f_id, media_body=media).execute()
        else: drive_service.files().create(body={'name': FILE_NAME_FOR_YEAR, 'parents': [DRIVE_FOLDER_ID], 'mimeType': 'text/csv'}, media_body=media).execute()
        
        # ë©”ì¼ìš© ë¶„ì„ ë¡œì§
        school_stats, innodep_today_dict, innodep_total_amt = {}, {}, 0
        for row in final_data:
            try:
                org, comp, amt_val, item_name, contract = str(row[7]), str(row[21]), str(row[20]), str(row[14]), str(row[23])
                amt = int(amt_val.replace(',', '').split('.')[0])
                if 'í•™êµ' in org and 'ì§€ëŠ¥í˜•' in contract and 'CCTV' in contract:
                    if org not in school_stats: school_stats[org] = {'total_amt': 0, 'main_vendor': '', 'vendor_priority': 3}
                    school_stats[org]['total_amt'] += amt
                    priority = 1 if 'ì˜ìƒê°ì‹œì¥ì¹˜' in item_name else 2 if 'ë³´ì•ˆìš©ì¹´ë©”ë¼' in item_name else 3
                    if priority < school_stats[org]['vendor_priority']:
                        school_stats[org]['main_vendor'], school_stats[org]['vendor_priority'] = comp, priority
                if 'ì´ë…¸ë' in comp:
                    innodep_today_dict[org] = innodep_today_dict.get(org, 0) + amt
                    innodep_total_amt += amt
            except: continue

        summary_lines = [f"â­ {d_str} í•™êµ ì§€ëŠ¥í˜• CCTV ë‚©í’ˆ í˜„í™©:"]
        for s, info in school_stats.items(): summary_lines.append(f"- {s} [{info['main_vendor']}]: {info['total_amt']:,}ì›")
        if not school_stats: summary_lines.append(" 0ê±´")
        
        summary_lines.extend([" ", f"ğŸ¢ {d_str} ì´ë…¸ë ì‹¤ì :"])
        for o, a in innodep_today_dict.items(): summary_lines.append(f"- {o}: {a:,}ì›")
        summary_lines.append(f"** ì´í•©ê³„: {innodep_total_amt:,}ì›") if innodep_today_dict else summary_lines.append(" 0ê±´")

        servc_html = fetch_and_generate_servc_html(target_dt)

        if "GITHUB_OUTPUT" in os.environ:
            with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
                f.write(f"collect_date={d_str}\ncollect_count={len(final_data)}\nschool_info<<EOF\n")
                for line in summary_lines: f.write(f"{line}<br>\n")
                f.write(f"EOF\nservc_info<<EOF\n{servc_html}\nEOF\n")

    # --- PART 2: ì…ì°° ê³µê³  ìˆ˜ì§‘ (G2B_notice í†µí•© ê¸°ëŠ¥) ---
    print(f"ğŸš€ ì…ì°° ê³µê³  ìˆ˜ì§‘ ì‹œì‘ ({d_str})")
    for cat, url in NOTICE_API_MAP.items():
        n_df = fetch_notice_data(cat, url, d_str)
        if not n_df.empty:
            f_name = f"ë‚˜ë¼ì¥í„°_ê³µê³ _{cat}.csv"
            res = drive_service.files().list(q=f"name='{f_name}' and '1N2GjNTpOvtn-5Vbg5zf6Y8kf4xuq0qTr' in parents and trashed=false", fields='files(id)').execute()
            items = res.get('files', [])
            fid = items[0]['id'] if items else None
            
            if fid:
                resp = requests.get(f'https://www.googleapis.com/drive/v3/files/{fid}?alt=media', headers={'Authorization': f'Bearer {drive_creds.token}'})
                if resp.status_code == 200:
                    n_df = pd.concat([pd.read_csv(io.BytesIO(resp.content), encoding='utf-8-sig', low_memory=False), n_df], ignore_index=True)
            
            n_df.drop_duplicates(subset=['bidNtceNo'], keep='last', inplace=True)
            media = MediaIoBaseUpload(io.BytesIO(n_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')), mimetype='text/csv', resumable=True)
            if fid: drive_service.files().update(fileId=fid, media_body=media).execute()
            else: drive_service.files().create(body={'name': f_name, 'parents': ['1N2GjNTpOvtn-5Vbg5zf6Y8kf4xuq0qTr'], 'mimeType': 'text/csv'}, media_body=media).execute()
            print(f"âœ… {cat} ê³µê³  ì—…ë°ì´íŠ¸ ì™„ë£Œ")

if __name__ == "__main__":
    main()
