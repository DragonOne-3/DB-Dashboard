import os, json, datetime, time, requests
import xml.etree.ElementTree as ET
import pandas as pd
import io
import threading
from googleapiclient.discovery import build
from google.oauth2 import service_account
from googleapiclient.http import MediaIoBaseUpload 
from pytimekr import pytimekr
import re

# ================= 1. ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜ =================
MY_DIRECT_KEY = os.environ.get('DATA_GO_KR_API_KEY')
AUTH_JSON_STR = os.environ.get('GOOGLE_AUTH_JSON')
save_lock = threading.Lock()

# ë¬¼í’ˆ ë‚©í’ˆ ìƒì„¸ ë‚´ì—­ìš© í—¤ë”
HEADER_KOR = ['ì¡°ë‹¬êµ¬ë¶„ëª…', 'ê³„ì•½êµ¬ë¶„ëª…', 'ê³„ì•½ë‚©í’ˆêµ¬ë¶„ëª…', 'ê³„ì•½ë‚©í’ˆìš”êµ¬ì¼ì', 'ê³„ì•½ë‚©í’ˆìš”êµ¬ë²ˆí˜¸', 'ë³€ê²½ì°¨ìˆ˜', 'ìµœì¢…ë³€ê²½ì°¨ìˆ˜ì—¬ë¶€', 'ìˆ˜ìš”ê¸°ê´€ëª…', 'ìˆ˜ìš”ê¸°ê´€êµ¬ë¶„ëª…', 'ìˆ˜ìš”ê¸°ê´€ì§€ì—­ëª…', 'ìˆ˜ìš”ê¸°ê´€ì½”ë“œ', 'ë¬¼í’ˆë¶„ë¥˜ë²ˆí˜¸', 'í’ˆëª…', 'ì„¸ë¶€ë¬¼í’ˆë¶„ë¥˜ë²ˆí˜¸', 'ì„¸ë¶€í’ˆëª…', 'ë¬¼í’ˆì‹ë³„ë²ˆí˜¸', 'ë¬¼í’ˆê·œê²©ëª…', 'ë‹¨ê°€', 'ìˆ˜ëŸ‰', 'ë‹¨ìœ„', 'ê¸ˆì•¡', 'ì—…ì²´ëª…', 'ì—…ì²´ê¸°ì—…êµ¬ë¶„ëª…', 'ê³„ì•½ëª…', 'ìš°ìˆ˜ì œí’ˆì—¬ë¶€', 'ê³µì‚¬ìš©ìì¬ì§ì ‘êµ¬ë§¤ëŒ€ìƒì—¬ë¶€', 'ë‹¤ìˆ˜ê³µê¸‰ìê³„ì•½ì—¬ë¶€', 'ë‹¤ìˆ˜ê³µê¸‰ìê³„ì•½2ë‹¨ê³„ì§„í–‰ì—¬ë¶€', 'ë‹¨ê°€ê³„ì•½ë²ˆí˜¸', 'ë‹¨ê°€ê³„ì•½ë³€ê²½ì°¨ìˆ˜', 'ìµœì´ˆê³„ì•½(ë‚©í’ˆìš”êµ¬)ì¼ì', 'ê³„ì•½ì²´ê²°ë°©ë²•ëª…', 'ì¦ê°ìˆ˜ëŸ‰', 'ì¦ê°ê¸ˆì•¡', 'ë‚©í’ˆì¥ì†Œëª…', 'ë‚©í’ˆê¸°í•œì¼ì', 'ì—…ì²´ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸', 'ì¸ë„ì¡°ê±´ëª…', 'ë¬¼í’ˆìˆœë²ˆ']

# ìˆ˜ì§‘ í‚¤ì›Œë“œ í†µí•© (ì¤‘ë³µ ì œê±° ë° ì •ë ¬)
keywords = sorted(list(set([
    'ë„¤íŠ¸ì›Œí¬ì‹œìŠ¤í…œì¥ë¹„ìš©ë™','ì˜ìƒê°ì‹œì¥ì¹˜','PAìš©ìŠ¤í”¼ì»¤','ì•ˆë‚´íŒ','ì¹´ë©”ë¼ë¸Œë˜í‚·','ì•¡ì •ëª¨ë‹ˆí„°','ê´‘ì†¡ìˆ˜ì‹ ëª¨ë“ˆ','ì „ì›ê³µê¸‰ì¥ì¹˜','ê´‘ë¶„ë°°í•¨','ì»¨ë²„í„°','ì»´í“¨í„°ì„œë²„','í•˜ë“œë””ìŠ¤í¬ë“œë¼ì´ë¸Œ','ë„¤íŠ¸ì›Œí¬ìŠ¤ìœ„ì¹˜','ê´‘ì í¼ì½”ë“œ','í’€ë°•ìŠ¤','ì„œì§€í¡ìˆ˜ê¸°','ë””ì§€í„¸ë¹„ë””ì˜¤ë ˆì½”ë”',
    'ìŠ¤í”¼ì»¤','ì˜¤ë””ì˜¤ì•°í”„','ë¸Œë˜í‚·','UTPì¼€ì´ë¸”','ì •ë³´í†µì‹ ê³µì‚¬','ì˜ìƒì •ë³´ë””ìŠ¤í”Œë ˆì´ì¥ì¹˜','ì†¡ì‹ ê¸°','ë‚œì—°ì „ë ¥ì¼€ì´ë¸”','1ì¢…ê¸ˆì†ì œê°€ìš”ì „ì„ ê´€','í˜¸ì˜¨ìŠ¤í”¼ì»¤','ëˆ„ì „ì°¨ë‹¨ê¸°','ë°©ì†¡ìˆ˜ì‹ ê¸°','LAPì™¸í”¼ê´‘ì¼€ì´ë¸”','í´ë¦¬ì—í‹¸ë Œì „ì„ ê´€','ë¦¬ëª¨íŠ¸ì•°í”„',
    'ë™ìºë¹„ë‹›ìš©íŒ¨ë„','ë² ì–´ë³¸ì»´í“¨í„°','ë¶„ë°°ê¸°','ê²°ì„ ë³´ë“œìœ ë‹›','ë²¨','ë‚œì—°ì ‘ì§€ìš©ë¹„ë‹ì ˆì—°ì „ì„ ','ê²½ê´‘ë“±','ë°ìŠ¤í¬í†±ì»´í“¨í„°','íŠ¹ìˆ˜ëª©ì ì»´í“¨í„°','ì² ê·¼ì½˜í¬ë¦¬íŠ¸ê³µì‚¬','í† ê³µì‚¬','ì•ˆë‚´ì „ê´‘íŒ','ì ‘ì§€ë´‰','ì¹´ë©”ë¼íšŒì „ëŒ€','ë¬´ì„ ëœì•¡ì„¸ìŠ¤í¬ì¸íŠ¸','ì»´í“¨í„°ë§ì „í™˜ì¥ì¹˜',
    'í¬ì¥ê³µì‚¬','ê³ ì£¼íŒŒë™ì¶•ì¼€ì´ë¸”','ì¹´ë©”ë¼í•˜ìš°ì§•','ì¸í„°í°','ìŠ¤ìœ„ì¹­ëª¨ë“œì „ì›ê³µê¸‰ì¥ì¹˜','ê¸ˆì†ìƒì','ì—´ì„ ê°ì§€ê¸°','íƒœì–‘ì „ì§€ì¡°ì ˆê¸°','ë°€íê³ ì •í˜•ë‚©ì¶•ì „ì§€','IPì „í™”ê¸°','ë””ìŠ¤í¬ì–´ë ˆì´','ê·¸ë˜í”½ìš©ì–´ëŒ‘í„°','ì¸í„°ì½¤ì¥ë¹„','ê¸°ì–µìœ ë‹›','ì»´í“¨í„°ì§€ë¬¸ì¸ì‹ì¥ì¹˜','ëœì ‘ì†ì¹´ë“œ',
    'ì ‘ì§€íŒ','ì œì–´ì¼€ì´ë¸”','ë¹„ë””ì˜¤ë„¤íŠ¸ì›Œí‚¹ì¥ë¹„','ë ˆì´ìŠ¤ì›¨ì´','ì½˜ì†”ìµìŠ¤í…ë”','ì „ìì¹´ë“œ','ë¹„ëŒ€ë©´ë°©ì—­ê°ì§€ì¥ë¹„','ì˜¨ìŠµë„íŠ¸ëœìŠ¤ë¯¸í„°','ë„ë‚œë°©ì§€ê¸°','ìœµë³µí•©ì˜ìƒê°ì‹œì¥ì¹˜','ë©€í‹°ìŠ¤í¬ë¦°ì»´í“¨í„°','ì»´í“¨í„°ì •ë§¥ì¸ì‹ì¥ì¹˜','ì¹´ë©”ë¼ì»¨íŠ¸ë¡¤ëŸ¬','SSDì €ì¥ì¥ì¹˜','ì›ê²©ë‹¨ë§ì¥ì¹˜(RTU)',
    'ìœµë³µí•©ë„¤íŠ¸ì›Œí¬ìŠ¤ìœ„ì¹˜','ìœµë³µí•©ì•¡ì •ëª¨ë‹ˆí„°','ìœµë³µí•©ë°ìŠ¤í¬í†±ì»´í“¨í„°','ìœµë³µí•©ê·¸ë˜í”½ìš©ì–´ëŒ‘í„°','ìœµë³µí•©ë² ì–´ë³¸ì»´í“¨í„°','ìœµë³µí•©ì„œì§€í¡ìˆ˜ê¸°','ë°°ì„ ì¥ì¹˜','ìœµë³µí•©ë°°ì„ ì¥ì¹˜','ìœµë³µí•©ì¹´ë©”ë¼ë¸Œë˜í‚·','ìœµë³µí•©ë„¤íŠ¸ì›Œí¬ì‹œìŠ¤í…œì¥ë¹„ìš©ë™','ìœµë³µí•©UTPì¼€ì´ë¸”','í…Œì´í”„ë°±ì—…ì¥ì¹˜',
    'ìê¸°ì‹í…Œì´í”„','ë ˆì´ë“œì €ì¥ì¥ì¹˜','ê´‘ì†¡ìˆ˜ì‹ ê¸°','450/750V ìœ ì—°ì„±ë‹¨ì‹¬ë¹„ë‹ì ˆì—°ì „ì„ ','ì†”ë‚´ì‹œìŠ¤í…œ','450/750Vìœ ì—°ì„±ë‹¨ì‹¬ë¹„ë‹ì ˆì—°ì „ì„ ','ì¹´ë©”ë¼ë°›ì¹¨ëŒ€','í…”ë ˆë¹„ì „ê±°ì¹˜ëŒ€','ê´‘ìˆ˜ì‹ ê¸°','ë¬´ì„ í†µì‹ ì¥ì¹˜','ë™ì‘ë¶„ì„ê¸°','ì „ë ¥ê³µê¸‰ì¥ì¹˜','450/750V ì¼ë°˜ìš©ìœ ì—°ì„±ë‹¨ì‹¬ë¹„ë‹ì ˆì—°ì „ì„ ','ë¶„ì „í•¨',
    'ë¹„ë””ì˜¤ë¯¹ì„œ','ì ˆì—°ì „ì„ ë°í”¼ë³µì„ ','ë ˆì´ë”','ì ì™¸ì„ ë°©ì‚¬ê¸°', 'ë³´ì•ˆìš©ì¹´ë©”ë¼', 'í†µì‹ ì†Œí”„íŠ¸ì›¨ì–´','ë¶„ì„ë°ê³¼í•™ìš©ì†Œí”„íŠ¸ì›¨ì–´','ì†Œí”„íŠ¸ì›¨ì–´ìœ ì§€ë°ì§€ì›ì„œë¹„ìŠ¤',
    'êµí†µê´€ì œì‹œìŠ¤í…œ', 'ì‚°ì—…ê´€ë¦¬ì†Œí”„íŠ¸ì›¨ì–´', 'ì‹œìŠ¤í…œê´€ë¦¬ì†Œí”„íŠ¸ì›¨ì–´', 'ì ì™¸ì„ ì¹´ë©”ë¼', 'ì£¼ì°¨ê²½ë³´ë“±', 'ì£¼ì°¨ê´€ì œì£¼ë³€ê¸°ê¸°', 'ì£¼ì°¨ê¶ŒíŒë…ê¸°', 'ì£¼ì°¨ì•ˆë‚´íŒ', 'ì£¼ì°¨ìš”ê¸ˆê³„ì‚°ê¸°', 'ì£¼ì°¨ì£¼ì œì–´ì¥ì¹˜', 'ì°¨ëŸ‰ê°ì§€ê¸°', 'ì°¨ëŸ‰ì¸ì‹ê¸°', 'ì°¨ëŸ‰ì°¨ë‹¨ê¸°', 'íŒ¨í‚¤ì§€ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œë°ë„ì…ì„œë¹„ìŠ¤', 'ë¬´ì„ ì¸ì‹ë¦¬ë”ê¸°', 'ë°”ì½”ë“œì‹œìŠ¤í…œ', 'ì¶œì…í†µì œì‹œìŠ¤í…œ', 'ì¹´ë“œì¸ì‡„ê¸°'
])))

NOTICE_API_MAP = {
    'ê³µì‚¬': 'https://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoCnstwkPPSSrch',
    'ë¬¼í’ˆ': 'https://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoThngPPSSrch',
    'ìš©ì—­': 'https://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoServcPPSSrch'
}

# ================= 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =================
def get_drive_service_for_script():
    info = json.loads(AUTH_JSON_STR)
    creds = service_account.Credentials.from_service_account_info(info, scopes=['https://www.googleapis.com/auth/drive'])
    return build('drive', 'v3', credentials=creds), creds

def get_target_date():
    now = datetime.datetime.utcnow() + datetime.timedelta(hours=9)
    target = now - datetime.timedelta(days=1)
    holidays = pytimekr.holidays(year=target.year)
    while target.weekday() >= 5 or target.date() in holidays:
        target -= datetime.timedelta(days=1)
    return target

def fetch_api_data_from_g2b(kw, d_str):
    url = "https://apis.data.go.kr/1230000/at/ShoppingMallPrdctInfoService/getSpcifyPrdlstPrcureInfoList"
    params = {'numOfRows': '999', 'pageNo': '1', 'ServiceKey': MY_DIRECT_KEY, 'Type_A': 'xml', 'inqryDiv': '1', 'inqryPrdctDiv': '2', 'inqryBgnDate': d_str, 'inqryEndDate': d_str, 'dtilPrdctClsfcNoNm': kw}
    try:
        res = requests.get(url, params=params, timeout=30)
        if res.status_code == 200 and "<item>" in res.text:
            root = ET.fromstring(res.content)
            return [[elem.text if elem.text else '' for elem in elem_item] for elem_item in root.findall('.//item')]
    except: pass
    return []

def fetch_and_generate_servc_html(target_dt):
    api_key = os.environ.get('DATA_GO_KR_API_KEY')
    api_url = 'http://apis.data.go.kr/1230000/ao/CntrctInfoService/getCntrctInfoListServcPPSSrch'
    target_date_str = target_dt.strftime("%Y%m%d")
    display_date_str = target_dt.strftime("%Y-%m-%d")
    keywords_servc = ['í†µí•©ê´€ì œ', 'CCTV', 'ì˜ìƒê°ì‹œì¥ì¹˜','êµ­ë°©','ê²½ê³„','ì‘ì „','ë¶€ëŒ€','ìœ¡êµ°','ê³µêµ°','í•´êµ°','ë¬´ì¸','ì£¼ì°¨','ì¶œì…','ê³¼í•™í™”','ì£¼ë‘”ì§€','ì¤‘ìš”ì‹œì„¤']
    collected_data = []

    for kw in keywords_servc:
        params = {'serviceKey': api_key, 'pageNo': '1', 'numOfRows': '999', 'inqryDiv': '1', 'type': 'xml', 'inqryBgnDate': target_date_str, 'inqryEndDate': target_date_str, 'cntrctNm': kw}
        try:
            res = requests.get(api_url, params=params, timeout=30)
            if res.status_code == 200:
                root = ET.fromstring(res.content)
                for item in root.findall('.//item'):
                    raw_demand = item.findtext('dminsttList', '-')
                    raw_corp = item.findtext('corpList', '-')
                    demand_parts = raw_demand.replace('[', '').replace(']', '').split('^')
                    clean_demand = demand_parts[2] if len(demand_parts) > 2 else raw_demand
                    corp_parts = raw_corp.replace('[', '').replace(']', '').split('^')
                    clean_corp = corp_parts[3] if len(corp_parts) > 3 else raw_corp
                    collected_data.append({
                        'demand': clean_demand, 'name': item.findtext('cntrctNm', '-'), 'corp': clean_corp,
                        'amount': int(item.findtext('totCntrctAmt', '0')), 'date': display_date_str
                    })
        except: pass

    unique_servc = {f"{d['demand']}_{d['name']}": d for d in collected_data}.values()
    html = f"<div style='margin-top: 20px;'><h4 style='color: #1a73e8; border-bottom: 2px solid #1a73e8; padding-bottom: 5px;'>ğŸ›ï¸ ë‚˜ë¼ì¥í„° ìš©ì—­ ê³„ì•½ ë‚´ì—­ ({display_date_str})</h4>"
    if not unique_servc:
        html += f"<p style='color: #666;'>- {display_date_str}ì— í•´ë‹¹ í‚¤ì›Œë“œ ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.</p></div>"
        return html

    html += "<table border='1' style='border-collapse: collapse; width: 100%; font-size: 11px;'> <tr style='background-color: #f8f9fa;'><th>ìˆ˜ìš”ê¸°ê´€</th><th>ê³„ì•½ëª…</th><th>ì—…ì²´ëª…</th><th>ê¸ˆì•¡</th></tr>"
    for row in unique_servc:
        bg = "background-color: #FFF9C4;" if "ì´ë…¸ë" in row['corp'] else ""
        html += f"<tr style='{bg}'><td>{row['demand']}</td><td>{row['name']}</td><td>{row['corp']}</td><td style='text-align: right;'>{row['amount']:,}ì›</td></tr>"
    html += "</table></div>"
    return html

def fetch_notice_data(category, url, d_str):
    params = {'serviceKey': MY_DIRECT_KEY, 'pageNo': '1', 'numOfRows': '999', 'inqryDiv': '1', 'type': 'json', 'inqryBgnDt': d_str + "0000", 'inqryEndDt': d_str + "2359"}
    try:
        res = requests.get(url, params=params, timeout=45)
        if res.status_code == 200:
            items = res.json().get('response', {}).get('body', {}).get('items', [])
            return pd.DataFrame(items)
    except: pass
    return pd.DataFrame()

# ================= 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§ =================
def main():
    if not MY_DIRECT_KEY or not AUTH_JSON_STR: return

    target_dt = get_target_date()
    d_str = target_dt.strftime("%Y%m%d")
    drive_service, drive_creds = get_drive_service_for_script()
    
    # --- PART 1: ë¬¼í’ˆ ë‚©í’ˆ ìƒì„¸ ë‚´ì—­ ìˆ˜ì§‘ ---
    final_data = []
    for kw in keywords:
        data = fetch_api_data_from_g2b(kw, d_str)
        if data: final_data.extend(data)
        time.sleep(0.5)
    
    summary_lines = []
    servc_html = ""

    if final_data:
        new_df = pd.DataFrame(final_data, columns=HEADER_KOR)
        FILE_NAME_FOR_YEAR = f"{target_dt.year}.csv"

        # ğŸš€ [ìˆ˜ì •] ì´ë¦„ìœ¼ë¡œë§Œ ê²€ìƒ‰ (ìµœìƒë‹¨ ê²€ìƒ‰ ëŒ€ì‘)
        query = f"name='{FILE_NAME_FOR_YEAR}' and trashed=false" 
        res = drive_service.files().list(q=query, fields='files(id)').execute()
        items = res.get('files', [])

        if items:
            f_id = items[0]['id']
            d_url = f'https://www.googleapis.com/drive/v3/files/{f_id}?alt=media'
            resp = requests.get(d_url, headers={'Authorization': f'Bearer {drive_creds.token}'})
            if resp.status_code == 200:
                old_df = pd.read_csv(io.BytesIO(resp.content), encoding='utf-8-sig', low_memory=False)
                df_to_upload = pd.concat([old_df, new_df], ignore_index=True).drop_duplicates(subset=['ê³„ì•½ë‚©í’ˆìš”êµ¬ì¼ì', 'ìˆ˜ìš”ê¸°ê´€ëª…', 'í’ˆëª…', 'ê¸ˆì•¡'], keep='last')
            else: df_to_upload = new_df
        else:
            f_id = None
            df_to_upload = new_df

        csv_bytes = df_to_upload.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
        media = MediaIoBaseUpload(io.BytesIO(csv_bytes), mimetype='text/csv', resumable=True)

        if f_id: drive_service.files().update(fileId=f_id, media_body=media).execute()
        else: print(f"âš ï¸ {FILE_NAME_FOR_YEAR} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ ìƒì„±ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ë©”ì¼ìš© ë¶„ì„ ë¡œì§
        school_stats, innodep_today_dict, innodep_total_amt = {}, {}, 0
        for row in final_data:
            try:
                org, comp, amt_val, item_nm, cntrct = str(row[7]), str(row[21]), str(row[20]), str(row[14]), str(row[23])
                amt = int(amt_val.replace(',', '').split('.')[0])
                if 'í•™êµ' in org and 'ì§€ëŠ¥í˜•' in cntrct and 'CCTV' in cntrct:
                    if org not in school_stats: school_stats[org] = {'total_amt': 0, 'main_vendor': '', 'vendor_priority': 3}
                    school_stats[org]['total_amt'] += amt
                    priority = 1 if 'ì˜ìƒê°ì‹œì¥ì¹˜' in item_nm else 2 if 'ë³´ì•ˆìš©ì¹´ë©”ë¼' in item_nm else 3
                    if priority < school_stats[org]['vendor_priority']:
                        school_stats[org]['main_vendor'], school_stats[org]['vendor_priority'] = comp, priority
                if 'ì´ë…¸ë' in comp:
                    innodep_today_dict[org] = innodep_today_dict.get(org, 0) + amt
                    innodep_total_amt += amt
            except: continue

        summary_lines = [f"â­ {d_str} í•™êµ ì§€ëŠ¥í˜• CCTV ë‚©í’ˆ í˜„í™©:"]
        for s, info in school_stats.items(): summary_lines.append(f"- {s} [{info['main_vendor']}]: {info['total_amt']:,}ì›")
        if not school_stats: summary_lines.append(" 0ê±´")
        
        summary_lines.extend([" ", f"ğŸ¢ {d_str} ì´ë…¸ë ì‹¤ì :"])
        for o, a in innodep_today_dict.items(): summary_lines.append(f"- {o}: {a:,}ì›")
        summary_lines.append(f"** ì´í•©ê³„: {innodep_total_amt:,}ì›") if innodep_today_dict else summary_lines.append(" 0ê±´")
        servc_html = fetch_and_generate_servc_html(target_dt)

    # --- PART 2: [í•„ìˆ˜ ìˆ˜ì •] ì…ì°° ê³µê³  ìˆ˜ì§‘ ë° ì£¼ìš” í‚¤ì›Œë“œ í•„í„°ë§ ---
    # --- PART 2: ì…ì°° ê³µê³  ìˆ˜ì§‘ ë° ì£¼ìš” í‚¤ì›Œë“œ í•„í„°ë§ (ìµœì¢… ë³´ì™„ë³¸) ---
    notice_mail_list = []
    
    # ğŸš€ [ì—…ë°ì´íŠ¸] ìš”ì²­í•˜ì‹  í™•ì¥ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    keywords_notice = [
        'CCTV', 'í†µí•©ê´€ì œ', 'ì˜ìƒê°ì‹œì¥ì¹˜', 'ì˜ìƒì •ë³´ì²˜ë¦¬ê¸°ê¸°', 'êµ­ë°©', 'ë¶€ëŒ€', 'ì‘ì „', 'ê²½ê³„', 'ë°©ìœ„',
        'ë°ì´í„°','í”Œë«í¼','ì†”ë£¨ì…˜','êµ°ì‚¬', 'ë¬´ì¸í™”', 'ì‚¬ë ¹ë¶€', 'êµ°ëŒ€','ìŠ¤ë§ˆíŠ¸ì‹œí‹°','ìŠ¤ë§ˆíŠ¸ë„ì‹œ','ITS','GIS',
        'ì¤‘ìš”ì‹œì„¤','ì£¼ë‘”ì§€','ê³¼í•™í™”','ì¶œì…','ì£¼ì°¨','ìœ¡êµ°','í•´êµ°','ê³µêµ°','í•´ë³‘'
    ]
    
    print(f"ğŸš€ ì…ì°° ê³µê³  ìˆ˜ì§‘ ì‹œì‘ ({d_str})")
    
    for cat, url in NOTICE_API_MAP.items():
        print(f"ğŸ“¡ [{cat}] API ìš”ì²­ ì¤‘...") # ğŸ” ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
        n_df = fetch_notice_data(cat, url, d_str)
        
        if n_df is None or n_df.empty:
            print(f"â“ [{cat}] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (API ì‘ë‹µ ë¹„ì–´ìˆìŒ)")
            continue
            
        print(f"ğŸ“¦ [{cat}] ìˆ˜ì§‘ ì„±ê³µ: {len(n_df)}ê±´")

        # 1. ë©”ì¼ìš© í•„í„°ë§
        pattern = '|'.join(keywords_notice)
        # ì»¬ëŸ¼ëª… ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ í•„í„°ë§ (ì•ˆì •ì„±)
        target_col = 'bidNtceNm' if 'bidNtceNm' in n_df.columns else n_df.columns[0] 
        filtered_n = n_df[n_df[target_col].str.contains(pattern, na=False, case=False)]
        
        print(f"ğŸ¯ [{cat}] í‚¤ì›Œë“œ í•„í„°ë§ ê²°ê³¼: {len(filtered_n)}ê±´ ë°œê²¬")

        for _, row in filtered_n.iterrows():
            notice_mail_list.append({
                'type': cat, 
                'org': row.get('dminsttNm', '-'), 
                'nm': row.get('bidNtceNm', '-'), 
                'url': row.get('bidNtceDtlUrl', '#')
            })

        # 2. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì €ì¥ (ìµœìƒë‹¨ ê²€ìƒ‰ ëŒ€ì‘)
        f_name = f"ë‚˜ë¼ì¥í„°_ê³µê³ _{cat}.csv"
        try:
            query_n = f"name='{f_name}' and trashed=false"
            res_n = drive_service.files().list(q=query_n, fields='files(id)', supportsAllDrives=True).execute()
            items_n = res_n.get('files', [])
            fid_n = items_n[0]['id'] if items_n else None
            
            if fid_n:
                resp_n = requests.get(f'https://www.googleapis.com/drive/v3/files/{fid_n}?alt=media', headers={'Authorization': f'Bearer {drive_creds.token}'})
                if resp_n.status_code == 200:
                    old_df_n = pd.read_csv(io.BytesIO(resp_n.content), encoding='utf-8-sig', low_memory=False)
                    n_df = pd.concat([old_df_n, n_df], ignore_index=True)
                
                # ì…ì°°ê³µê³ ë²ˆí˜¸(bidNtceNo) ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
                if 'bidNtceNo' in n_df.columns:
                    n_df.drop_duplicates(subset=['bidNtceNo'], keep='last', inplace=True)
                
                csv_out = n_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                media_n = MediaIoBaseUpload(io.BytesIO(csv_out), mimetype='text/csv', resumable=True)
                drive_service.files().update(fileId=fid_n, media_body=media_n, supportsAllDrives=True).execute()
                print(f"âœ… [{cat}] ê³µê³  êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            else:
                print(f"âš ï¸ [{cat}] {f_name} íŒŒì¼ì´ ë“œë¼ì´ë¸Œì— ì—†ìŠµë‹ˆë‹¤. ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ [{cat}] ë“œë¼ì´ë¸Œ ì €ì¥ ì¤‘ ì—ëŸ¬: {e}")

    # ë©”ì¼ìš© ê³µê³  HTML ìƒì„±
    notice_html = f"<div style='margin-top: 20px;'><h4 style='color: #d32f2f; border-bottom: 2px solid #d32f2f; padding-bottom: 5px;'>ğŸ“¢ ì£¼ìš” í‚¤ì›Œë“œ ì…ì°° ê³µê³  ({d_str})</h4>"
    if not notice_mail_list:
        notice_html += f"<p style='color: #666;'>- {d_str}ì— í•´ë‹¹ í‚¤ì›Œë“œ ê³µê³  ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.</p></div>"
    else:
        notice_html += "<table border='1' style='border-collapse: collapse; width: 100%; font-size: 11px;'> <tr style='background-color: #f8f9fa;'><th>êµ¬ë¶„</th><th>ìˆ˜ìš”ê¸°ê´€</th><th>ê³µê³ ëª…(ë§í¬)</th></tr>"
        for n in notice_mail_list:
            notice_html += f"<tr><td style='text-align:center;'>{n['type']}</td><td>{n['org']}</td><td><a href='{n['url']}'>{n['nm']}</a></td></tr>"
        notice_html += "</table></div>"

    # --- PART 3: GitHub Actions Output ì„¤ì • ---
    if "GITHUB_OUTPUT" in os.environ:
        with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
            f.write(f"collect_date={d_str}\n")
            f.write(f"collect_count={len(final_data)}\n")
            f.write(f"notice_info<<EOF\n{notice_html}\nEOF\n")
            f.write(f"school_info<<EOF\n")
            for line in summary_lines: f.write(f"{line}<br>\n")
            f.write(f"EOF\n")
            f.write(f"servc_info<<EOF\n{servc_html}\nEOF\n")

if __name__ == "__main__":
    main()
