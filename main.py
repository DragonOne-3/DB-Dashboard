import os, json, datetime, time, requests

import xml.etree.ElementTree as ET

# êµ¬ê¸€ ë“œë¼ì´ë¸Œ CSV ì €ì¥ì„ ìœ„í•´ í•„ìš”í•œ ëª¨ë“ˆë“¤
import pandas as pd
import io
from googleapiclient.discovery import build
from google.oauth2 import service_account
from googleapiclient.http import MediaIoBaseUpload 

# ê¸°íƒ€ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ í•„ìš”í•œ ëª¨ë“ˆë“¤
from pytimekr import pytimekr # ê³µíœ´ì¼ ê³„ì‚°ìš©
import re # ì •ê·œì‹ ì‚¬ìš©ì„ ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.


# [ë³´ì•ˆ ì ìš©] í™˜ê²½ ë³€ìˆ˜ì—ì„œ í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
MY_DIRECT_KEY = os.environ.get('DATA_GO_KR_API_KEY')
AUTH_JSON_STR = os.environ.get('GOOGLE_AUTH_JSON')


# êµ­ë¬¸ í—¤ë” (43ê°œ í•­ëª© ì „ì²´ ìœ ì§€)
HEADER_KOR = ['ì¡°ë‹¬êµ¬ë¶„ëª…', 'ê³„ì•½êµ¬ë¶„ëª…', 'ê³„ì•½ë‚©í’ˆêµ¬ë¶„ëª…', 'ê³„ì•½ë‚©í’ˆìš”êµ¬ì¼ì', 'ê³„ì•½ë‚©í’ˆìš”êµ¬ë²ˆí˜¸', 'ë³€ê²½ì°¨ìˆ˜', 'ìµœì¢…ë³€ê²½ì°¨ìˆ˜ì—¬ë¶€', 'ìˆ˜ìš”ê¸°ê´€ëª…', 'ìˆ˜ìš”ê¸°ê´€êµ¬ë¶„ëª…', 'ìˆ˜ìš”ê¸°ê´€ì§€ì—­ëª…', 'ìˆ˜ìš”ê¸°ê´€ì½”ë“œ', 'ë¬¼í’ˆë¶„ë¥˜ë²ˆí˜¸', 'í’ˆëª…', 'ì„¸ë¶€ë¬¼í’ˆë¶„ë¥˜ë²ˆí˜¸', 'ì„¸ë¶€í’ˆëª…', 'ë¬¼í’ˆì‹ë³„ë²ˆí˜¸', 'ë¬¼í’ˆê·œê²©ëª…', 'ë‹¨ê°€', 'ìˆ˜ëŸ‰', 'ë‹¨ìœ„', 'ê¸ˆì•¡', 'ì—…ì²´ëª…', 'ì—…ì²´ê¸°ì—…êµ¬ë¶„ëª…', 'ê³„ì•½ëª…', 'ìš°ìˆ˜ì œí’ˆì—¬ë¶€', 'ê³µì‚¬ìš©ìì¬ì§ì ‘êµ¬ë§¤ëŒ€ìƒì—¬ë¶€', 'ë‹¤ìˆ˜ê³µê¸‰ìê³„ì•½ì—¬ë¶€', 'ë‹¤ìˆ˜ê³µê¸‰ìê³„ì•½2ë‹¨ê³„ì§„í–‰ì—¬ë¶€', 'ë‹¨ê°€ê³„ì•½ë²ˆí˜¸', 'ë‹¨ê°€ê³„ì•½ë³€ê²½ì°¨ìˆ˜', 'ìµœì´ˆê³„ì•½(ë‚©í’ˆìš”êµ¬)ì¼ì', 'ê³„ì•½ì²´ê²°ë°©ë²•ëª…', 'ì¦ê°ìˆ˜ëŸ‰', 'ì¦ê°ê¸ˆì•¡', 'ë‚©í’ˆì¥ì†Œëª…', 'ë‚©í’ˆê¸°í•œì¼ì', 'ì—…ì²´ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸', 'ì¸ë„ì¡°ê±´ëª…', 'ë¬¼í’ˆìˆœë²ˆ']


# ìš”ì²­í•˜ì‹  ëª¨ë“  í‚¤ì›Œë“œ í’€ ë¦¬ìŠ¤íŠ¸
keywords = [
    'ë„¤íŠ¸ì›Œí¬ì‹œìŠ¤í…œì¥ë¹„ìš©ë™','ì˜ìƒê°ì‹œì¥ì¹˜','PAìš©ìŠ¤í”¼ì»¤','ì•ˆë‚´íŒ','ì¹´ë©”ë¼ë¸Œë˜í‚·','ì•¡ì •ëª¨ë‹ˆí„°','ê´‘ì†¡ìˆ˜ì‹ ëª¨ë“ˆ','ì „ì›ê³µê¸‰ì¥ì¹˜','ê´‘ë¶„ë°°í•¨','ì»¨ë²„í„°','ì»´í“¨í„°ì„œë²„','í•˜ë“œë””ìŠ¤í¬ë“œë¼ì´ë¸Œ','ë„¤íŠ¸ì›Œí¬ìŠ¤ìœ„ì¹˜','ê´‘ì í¼ì½”ë“œ','í’€ë°•ìŠ¤','ì„œì§€í¡ìˆ˜ê¸°','ë””ì§€í„¸ë¹„ë””ì˜¤ë ˆì½”ë”',
    'ìŠ¤í”¼ì»¤','ì˜¤ë””ì˜¤ì•°í”„','ë¸Œë˜í‚·','UTPì¼€ì´ë¸”','ì •ë³´í†µì‹ ê³µì‚¬','ì˜ìƒì •ë³´ë””ìŠ¤í”Œë ˆì´ì¥ì¹˜','ì†¡ì‹ ê¸°','ë‚œì—°ì „ë ¥ì¼€ì´ë¸”','1ì¢…ê¸ˆì†ì œê°€ìš”ì „ì„ ê´€','í˜¸ì˜¨ìŠ¤í”¼ì»¤','ëˆ„ì „ì°¨ë‹¨ê¸°','ë°©ì†¡ìˆ˜ì‹ ê¸°','LAPì™¸í”¼ê´‘ì¼€ì´ë¸”','í´ë¦¬ì—í‹¸ë Œì „ì„ ê´€','ë¦¬ëª¨íŠ¸ì•°í”„',
    'ë™ìºë¹„ë‹›ìš©íŒ¨ë„','ë² ì–´ë³¸ì»´í“¨í„°','ë¶„ë°°ê¸°','ê²°ì„ ë³´ë“œìœ ë‹›','ë²¨','ë‚œì—°ì ‘ì§€ìš©ë¹„ë‹ì ˆì—°ì „ì„ ','ê²½ê´‘ë“±','ë°ìŠ¤í¬í†±ì»´í“¨í„°','íŠ¹ìˆ˜ëª©ì ì»´í“¨í„°','ì² ê·¼ì½˜í¬ë¦¬íŠ¸ê³µì‚¬','í† ê³µì‚¬','ì•ˆë‚´ì „ê´‘íŒ','ì ‘ì§€ë´‰','ì¹´ë©”ë¼íšŒì „ëŒ€','ë¬´ì„ ëœì•¡ì„¸ìŠ¤í¬ì¸íŠ¸','ì»´í“¨í„°ë§ì „í™˜ì¥ì¹˜',
    'í¬ì¥ê³µì‚¬','ê³ ì£¼íŒŒë™ì¶•ì¼€ì´ë¸”','ì¹´ë©”ë¼í•˜ìš°ì§•','ì¸í„°í°','ìŠ¤ìœ„ì¹­ëª¨ë“œì „ì›ê³µê¸‰ì¥ì¹˜','ê¸ˆì†ìƒì','ì—´ì„ ê°ì§€ê¸°','íƒœì–‘ì „ì§€ì¡°ì ˆê¸°','ë°€íê³ ì •í˜•ë‚©ì¶•ì „ì§€','IPì „í™”ê¸°','ë””ìŠ¤í¬ì–´ë ˆì´','ê·¸ë˜í”½ìš©ì–´ëŒ‘í„°','ì¸í„°ì½¤ì¥ë¹„','ê¸°ì–µìœ ë‹›','ì»´í“¨í„°ì§€ë¬¸ì¸ì‹ì¥ì¹˜','ëœì ‘ì†ì¹´ë“œ',
    'ì ‘ì§€íŒ','ì œì–´ì¼€ì´ë¸”','ë¹„ë””ì˜¤ë„¤íŠ¸ì›Œí‚¹ì¥ë¹„','ë ˆì´ìŠ¤ì›¨ì´','ì½˜ì†”ìµìŠ¤í…ë”','ì „ìì¹´ë“œ','ë¹„ëŒ€ë©´ë°©ì—­ê°ì§€ì¥ë¹„','ì˜¨ìŠµë„íŠ¸ëœìŠ¤ë¯¸í„°','ë„ë‚œë°©ì§€ê¸°','ìœµë³µí•©ì˜ìƒê°ì‹œì¥ì¹˜','ë©€í‹°ìŠ¤í¬ë¦°ì»´í“¨í„°','ì»´í“¨í„°ì •ë§¥ì¸ì‹ì¥ì¹˜','ì¹´ë©”ë¼ì»¨íŠ¸ë¡¤ëŸ¬','SSDì €ì¥ì¥ì¹˜','ì›ê²©ë‹¨ë§ì¥ì¹˜(RTU)',

    'ìœµë³µí•©ë„¤íŠ¸ì›Œí¬ìŠ¤ìœ„ì¹˜','ìœµë³µí•©ì•¡ì •ëª¨ë‹ˆí„°','ìœµë³µí•©ë°ìŠ¤í¬í†±ì»´í“¨í„°','ìœµë³µí•©ê·¸ë˜í”½ìš©ì–´ëŒ‘í„°','ìœµë³µí•©ë² ì–´ë³¸ì»´í“¨í„°','ìœµë³µí•©ì„œì§€í¡ìˆ˜ê¸°','ë°°ì„ ì¥ì¹˜','ìœµë³µí•©ë°°ì„ ì¥ì¹˜','ìœµë³µí•©ì¹´ë©”ë¼ë¸Œë˜í‚·','ìœµë³µí•©ë„¤íŠ¸ì›Œí¬ì‹œìŠ¤í…œì¥ë¹„ìš©ë™','ìœµë³µí•©UTPì¼€ì´ë¸”','í…Œì´í”„ë°±ì—…ì¥ì¹˜',
    'ìê¸°ì‹í…Œì´í”„','ë ˆì´ë“œì €ì¥ì¥ì¹˜','ê´‘ì†¡ìˆ˜ì‹ ê¸°','450/750V ìœ ì—°ì„±ë‹¨ì‹¬ë¹„ë‹ì ˆì—°ì „ì„ ','ì†”ë‚´ì‹œìŠ¤í…œ','450/750Vìœ ì—°ì„±ë‹¨ì‹¬ë¹„ë‹ì ˆì—°ì „ì„ ','ì¹´ë©”ë¼ë°›ì¹¨ëŒ€','í…”ë ˆë¹„ì „ê±°ì¹˜ëŒ€','ê´‘ìˆ˜ì‹ ê¸°','ë¬´ì„ í†µì‹ ì¥ì¹˜','ë™ì‘ë¶„ì„ê¸°','ì „ë ¥ê³µê¸‰ì¥ì¹˜','450/750V ì¼ë°˜ìš©ìœ ì—°ì„±ë‹¨ì‹¬ë¹„ë‹ì ˆì—°ì „ì„ ','ë¶„ì „í•¨',
    'ë¹„ë””ì˜¤ë¯¹ì„œ','ì ˆì—°ì „ì„ ë°í”¼ë³µì„ ','ë ˆì´ë”','ì ì™¸ì„ ë°©ì‚¬ê¸°', 'ë³´ì•ˆìš©ì¹´ë©”ë¼', 'í†µì‹ ì†Œí”„íŠ¸ì›¨ì–´','ë¶„ì„ë°ê³¼í•™ìš©ì†Œí”„íŠ¸ì›¨ì–´','ì†Œí”„íŠ¸ì›¨ì–´ìœ ì§€ë°ì§€ì›ì„œë¹„ìŠ¤'
]

# êµ¬ê¸€ ë“œë¼ì´ë¸Œ API ì„œë¹„ìŠ¤ í•¨ìˆ˜ (ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ ì „ìš©)
def get_drive_service_for_script():
    info = json.loads(AUTH_JSON_STR)
    # ğŸ’¡ [ë£¨ì´íŠ¼ ë°˜ì˜] (ìŠ¤ì½”í”„ í™•ì¥) drive.file ëŒ€ì‹  drive ìŠ¤ì½”í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì½ê¸°/ì“°ê¸° ê¶Œí•œì„ ë„“í™ë‹ˆë‹¤.
    scopes = ['https://www.googleapis.com/auth/drive'] # drive.file -> drive ë¡œ ë³€ê²½!
    creds = service_account.Credentials.from_service_account_info(info, scopes=scopes)
    return build('drive', 'v3', credentials=creds), creds


# (AttributeError í•´ê²°) get_target_date í•¨ìˆ˜
def get_target_date():
    """í•œêµ­ ì‹œê°„ ê¸°ì¤€, ê³µíœ´ì¼ ì œì™¸ ìµœê·¼ í‰ì¼ ê³„ì‚°"""
    now = datetime.datetime.utcnow() + datetime.timedelta(hours=9)
    target = now - datetime.timedelta(days=1)
    
    holidays = pytimekr.holidays(year=target.year)
    
    while target.weekday() >= 5 or target.date() in holidays:
        target -= datetime.timedelta(days=1)
    return target


def get_quarter(month):
    return (month - 1) // 3 + 1


# ë°ì´í„° APIë¡œë¶€í„° ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def fetch_api_data_from_g2b(kw, d_str):
    url = "https://apis.data.go.kr/1230000/at/ShoppingMallPrdctInfoService/getSpcifyPrdlstPrcureInfoList"
    params = {'numOfRows': '999', 'pageNo': '1', 'ServiceKey': MY_DIRECT_KEY, 'Type_A': 'xml', 'inqryDiv': '1', 'inqryPrdctDiv': '2', 'inqryBgnDate': d_str, 'inqryEndDate': d_str, 'dtilPrdctClsfcNoNm': kw}
    try:
        res = requests.get(url, params=params, timeout=30)
        if res.status_code == 200 and "<item>" in res.text:
            root = ET.fromstring(res.content)
            return [[elem.text if elem.text else '' for elem in elem_item] for elem_item in root.findall('.//item')]
    except Exception as e:
        print(f"Error fetching data for keyword '{kw}' on date '{d_str}': {e}")
        pass
    return []


def fetch_and_generate_servc_html(target_dt):
    """ìš©ì—­ ê³„ì•½ ë‚´ì—­ ìˆ˜ì§‘ ë° HTML ìƒì„±"""
    api_key = os.environ.get('DATA_GO_KR_API_KEY')
    api_url = 'http://apis.data.go.kr/1230000/ao/CntrctInfoService/getCntrctInfoListServcPPSSrch'
    target_date_str = target_dt.strftime("%Y%m%d")
    display_date_str = target_dt.strftime("%Y-%m-%d")
    
    keywords_servc = ['í†µí•©ê´€ì œ', 'CCTV', 'ì˜ìƒê°ì‹œì¥ì¹˜','êµ­ë°©','ê²½ê³„','ì‘ì „','ë¶€ëŒ€','ìœ¡êµ°','ê³µêµ°','í•´êµ°','ë¬´ì¸']
    collected_data = []

    for kw in keywords_servc:
        params = {'serviceKey': api_key, 'pageNo': '1', 'numOfRows': '999', 'inqryDiv': '1', 'type': 'xml', 'inqryBgnDate': target_date_str, 'inqryEndDate': target_date_str, 'cntrctNm': kw}
        try:
            res = requests.get(api_url, params=params, timeout=30)
            if res.status_code == 200:
                root = ET.fromstring(res.content)
                items = root.findall('.//item')
                for item in items:
                    raw_demand = item.findtext('dminsttList', '-')
                    raw_corp = item.findtext('corpList', '-')
                    demand_parts = raw_demand.replace('[', '').replace(']', '').split('^')
                    clean_demand = demand_parts[2] if len(demand_parts) > 2 else raw_demand
                    corp_parts = raw_corp.replace('[', '').replace(']', '').split('^')
                    clean_corp = corp_parts[3] if len(corp_parts) > 3 else raw_corp
                    
                    collected_data.append({
                        'demand': clean_demand, 'name': item.findtext('cntrctNm', '-'), 'corp': clean_corp,
                        'amount': int(item.findtext('totCntrctAmt', '0')), 'date': target_dt.strftime("%Y-%m-%d"),
                        'end_date': item.findtext('ttalScmpltDate', '-')
                    })
        except Exception as e:
            print(f"Error fetching service data for keyword '{kw}': {e}")
            pass # ì—ëŸ¬ ë°œìƒ ì‹œ ê±´ë„ˆë›°ê¸°

    unique_servc = {f"{d['demand']}_{d['name']}": d for d in collected_data}.values()
    
    html = f"<div style='margin-top: 20px;'><h4 style='color: #1a73e8; border-bottom: 2px solid #1a73e8; padding-bottom: 5px;'>ğŸ›ï¸ ë‚˜ë¼ì¥í„° ìš©ì—­ ê³„ì•½ ë‚´ì—­ ({display_date_str})</h4>"
    if not unique_servc:
        html += f"<p style='color: #666;'>- {display_date_str}ì— í•´ë‹¹ í‚¤ì›Œë“œ ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.</p></div>"
        return html

    html += "<table border='1' style='border-collapse: collapse; width: 100%; font-size: 11px;'> <tr style='background-color: #f8f9fa;'><th>ìˆ˜ìš”ê¸°ê´€</th><th>ê³„ì•½ëª…</th><th>ì—…ì²´ëª…</th><th>ê¸ˆì•¡</th></tr>"
    for row in unique_servc:
        bg = "background-color: #FFF9C4;" if "ì´ë…¸ë" in row['corp'] else ""
        html += f"<tr style='{bg}'><td>{row['demand']}</td><td>{row['name']}</td><td>{row['corp']}</td><td style='text-align: right;'>{row['amount']:,}ì›</td></tr>"
    html += "</table></div>"
    return html


def main():
    if not MY_DIRECT_KEY or not AUTH_JSON_STR:
        print("âŒ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½ (DATA_GO_KR_API_KEY ë˜ëŠ” GOOGLE_AUTH_JSON)"); return

    target_dt = get_target_date()
    d_str = target_dt.strftime("%Y%m%d")
    
    # êµ¬ê¸€ ë“œë¼ì´ë¸Œ API ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    drive_service, drive_creds = get_drive_service_for_script()
    
    final_data = []
    for kw in keywords:
        data = fetch_api_data_from_g2b(kw, d_str)
        if data: final_data.extend(data)
        time.sleep(0.5) # API í˜¸ì¶œ ê°„ ë”œë ˆì´
    
    if final_data:
        # ì´ ë¶€ë¶„ì´ êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ ë¡œì§ ëŒ€ì‹  êµ¬ê¸€ ë“œë¼ì´ë¸Œ CSV íŒŒì¼ ì €ì¥ ë¡œì§ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.
        
        # 0. ìˆ˜ì§‘ëœ final_dataë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        new_df = pd.DataFrame(final_data, columns=HEADER_KOR)
        
        # --- íŒŒì¼ ì •ë³´ ì„¤ì • ---
        DRIVE_FOLDER_ID = '1N2GjNTpOvtn-5Vbg5zf6Y8kf4xuq0qTr' # ë‹¹ì‹ ì´ ì§€ì •í•œ êµ¬ê¸€ ë“œë¼ì´ë¸Œ í´ë” ID!
        FILE_NAME_FOR_YEAR = f"{target_dt.year}.csv"          # ì €ì¥í•  CSV íŒŒì¼ ì´ë¦„ (ì˜ˆ: 2026.csv)

        # --- ë””ë²„ê¹… ì •ë³´ ---
        print(f"DEBUG: ìŠ¤í¬ë¦½íŠ¸ê°€ ì‚¬ìš©í•˜ë ¤ëŠ” í´ë” ID: '{DRIVE_FOLDER_ID}'")
        print(f"DEBUG: ìŠ¤í¬ë¦½íŠ¸ê°€ ì°¾ìœ¼ë ¤ëŠ” íŒŒì¼ëª…: '{FILE_NAME_FOR_YEAR}'")
        # --- ë””ë²„ê¹… ì •ë³´ ë ---

        # --- êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ í•´ë‹¹ ì—°ë„ CSV íŒŒì¼ ì°¾ê¸° ---
        file_id = None
        
        # Drive APIë¡œ íƒ€ê²Ÿ í´ë” ë‚´ì—ì„œ íŒŒì¼ ê²€ìƒ‰
        # mimeType='text/csv'ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤. ë§Œì•½ íŒŒì¼ ìœ í˜•ì´ 'Google ìŠ¤í”„ë ˆë“œì‹œíŠ¸'ë¼ë©´ ì´ ì¿¼ë¦¬ë¡œëŠ” ì°¾ì§€ ëª»í•©ë‹ˆë‹¤.
        # ì´ ê²½ìš° í•´ë‹¹ íŒŒì¼ì„ ì‹¤ì œ CSV íŒŒì¼ë¡œ ì—…ë¡œë“œí•˜ê±°ë‚˜, mimeTypeì„ 'application/vnd.google-apps.spreadsheet'ë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.
        query = f"name='{FILE_NAME_FOR_YEAR}' and '{DRIVE_FOLDER_ID}' in parents and trashed=false and mimeType='text/csv'" 
        print(f"DEBUG: Drive API ì‹¤í–‰ ì¿¼ë¦¬: '{query}'") # ì´ ì¿¼ë¦¬ ë¬¸ìì—´ë„ ì¤‘ìš”!
        results = drive_service.files().list(q=query, spaces='drive', fields='files(id)').execute()
        items = results.get('files', [])

        print(f"DEBUG: Drive API ì¿¼ë¦¬ ê²°ê³¼ items: {items}") # itemsê°€ ë¹„ì–´ìˆìœ¼ë©´ []ê°€ ì¶œë ¥ë  ê²ƒì…ë‹ˆë‹¤.
        
        if items: # íŒŒì¼ì´ ì¡´ì¬í•  ê²½ìš° (itemsê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°)
            file_id = items[0]['id']
            print(f"ğŸ”„ ê¸°ì¡´ íŒŒì¼ '{FILE_NAME_FOR_YEAR}' (ID: {file_id})ì— ë°ì´í„° ì¶”ê°€ ì¤‘...")
            
            # ê¸°ì¡´ íŒŒì¼ ë‚´ìš© ë‹¤ìš´ë¡œë“œ
            download_url = f'https://www.googleapis.com/drive/v3/files/{file_id}?alt=media'
            response = requests.get(download_url, headers={'Authorization': f'Bearer {drive_creds.token}'})
            
            if response.status_code == 200:
                # ë‹¤ìš´ë¡œë“œ ë°›ì€ CSV íŒŒì¼ì„ DataFrameìœ¼ë¡œ ì½ê¸° (í•œê¸€ ì¸ì½”ë”© 'utf-8-sig' ê³ ë ¤)
                existing_df = pd.read_csv(io.BytesIO(response.content), encoding='utf-8-sig', low_memory=False)
                
                # ê¸°ì¡´ ë°ì´í„° ë°‘ì— ìƒˆë¡œìš´ ë°ì´í„°(new_df)ë¥¼ ì¶”ê°€
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                
                # 'ì œì¼ ë§ˆì§€ë§‰ ë°ì´í„° ë°‘ì— ì¶”ê°€'í•˜ë˜, í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ì£¼ìš” ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.
                # `keep='last'`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„°(ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ ë°ì´í„°)ê°€ ìœ ì§€ë˜ë„ë¡ í•©ë‹ˆë‹¤.
                deduplicated_combined_df = combined_df.drop_duplicates(
                    subset=['ê³„ì•½ë‚©í’ˆìš”êµ¬ì¼ì', 'ìˆ˜ìš”ê¸°ê´€ëª…', 'í’ˆëª…', 'ê¸ˆì•¡'], 
                    keep='last'
                )
                df_to_upload = deduplicated_combined_df
                print(f"âœ… ê¸°ì¡´ '{FILE_NAME_FOR_YEAR}' ë°ì´í„° {len(existing_df)}ê±´ì— ì˜¤ëŠ˜ ë°ì´í„° {len(new_df)}ê±´ ì¶”ê°€ (ì¤‘ë³µ ì œê±° í›„ ìµœì¢… {len(df_to_upload)}ê±´).")
            else:
                print(f"âš ï¸ ê¸°ì¡´ íŒŒì¼ '{FILE_NAME_FOR_YEAR}' ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ìƒíƒœ ì½”ë“œ: {response.status_code}). ì˜¤ëŠ˜ ë°ì´í„°ë§Œìœ¼ë¡œ íŒŒì¼ ì—…ë°ì´íŠ¸/ìƒì„± ì‹œë„.")
                df_to_upload = new_df # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ ìƒˆ ë°ì´í„°ë§Œìœ¼ë¡œ ì—…ë¡œë“œ
        else: # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° (itemsê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°)
            print(f"ğŸ†• íŒŒì¼ '{FILE_NAME_FOR_YEAR}'ì´(ê°€) ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤. ì˜¤ëŠ˜ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
            df_to_upload = new_df

        # --- ì—…ë°ì´íŠ¸/ìƒì„±í•  CSV ë°ì´í„°ë¥¼ ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³€í™˜ ---
        csv_buffer = io.StringIO()
        df_to_upload.to_csv(csv_buffer, index=False, encoding='utf-8-sig') # í•œê¸€ ê¹¨ì§ ë°©ì§€ë¥¼ ìœ„í•´ 'utf-8-sig'
        csv_bytes = csv_buffer.getvalue().encode('utf-8-sig') # BytesIOì— ë„£ê¸° ìœ„í•´ ë‹¤ì‹œ ë°”ì´íŠ¸ë¡œ ì¸ì½”ë”©

        # --- êµ¬ê¸€ ë“œë¼ì´ë¸Œì— íŒŒì¼ ì—…ë¡œë“œ/ì—…ë°ì´íŠ¸ ---
        # media_bodyë¥¼ MediaIoBaseUploadë¡œ ê°ì‹¸ì¤ë‹ˆë‹¤.
        media_body = MediaIoBaseUpload(io.BytesIO(csv_bytes), mimetype='text/csv', resumable=True)

        if file_id: # ê¸°ì¡´ íŒŒì¼ ì—…ë°ì´íŠ¸
            drive_service.files().update(
                fileId=file_id,
                media_body=media_body, 
            ).execute()
            print(f"âœ… '{FILE_NAME_FOR_YEAR}' ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
        else: # ìƒˆ íŒŒì¼ ìƒì„±
            file_metadata = {
                'name': FILE_NAME_FOR_YEAR,
                'parents': [DRIVE_FOLDER_ID], 
                'mimeType': 'text/csv'
            }
            drive_service.files().create(
                body=file_metadata,
                media_body=media_body, 
                fields='id' # ìƒì„±ëœ íŒŒì¼ IDë¥¼ ë°›ê¸° ìœ„í•¨
            ).execute()
            print(f"âœ… '{FILE_NAME_FOR_YEAR}' ìƒì„± ë° ì—…ë¡œë“œ ì™„ë£Œ!")
        
        print(f"âœ… {d_str} ì›ë³¸ ë°ì´í„° {len(final_data)}ê±´ CSV íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ.")

        # --- 2. [ë¶„ì„ ë° ë©”ì¸ ë³¸ë¬¸ìš©] ì¤‘ë³µ ì œê±° ë¡œì§ ---
        unique_final_data = {} 
        for row in final_data: 
            try:
                key = (str(row[7]), str(row[21]), str(row[20]), str(row[14]))
                if key not in unique_final_data:
                    unique_final_data[key] = row
            except IndexError: continue
        
        deduplicated_data = list(unique_final_data.values())

        school_stats = {} 
        innodep_today_dict = {} 
        innodep_total_amt = 0

        # 3. ì¤‘ë³µ ì œê±°ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½ ë¶„ì„
        for row in deduplicated_data:
            try:
                org_name = str(row[7])
                item_name = str(row[14])
                amt_val = str(row[20])
                comp_name = str(row[21])
                contract_name = str(row[23])
                amt_raw = amt_val.replace(',', '').split('.')[0]
                amt = int(amt_raw) if amt_raw else 0
            except: continue

            # í•™êµ ì§€ëŠ¥í˜• CCTV ë¶„ì„
            if 'í•™êµ' in org_name and 'ì§€ëŠ¥í˜•' in contract_name and 'CCTV' in contract_name:
                if org_name not in school_stats:
                    school_stats[org_name] = {'total_amt': 0, 'main_vendor': '', 'vendor_priority': 3}
                school_stats[org_name]['total_amt'] += amt
                priority = 1 if 'ì˜ìƒê°ì‹œì¥ì¹˜' in item_name else 2 if 'ë³´ì•ˆìš©ì¹´ë©”ë¼' in item_name else 3
                if priority < school_stats[org_name]['vendor_priority']:
                    school_stats[org_name]['main_vendor'] = comp_name
                    school_stats[org_name]['vendor_priority'] = priority

            # ì´ë…¸ë ì‹¤ì  í•©ì‚°
            if 'ì´ë…¸ë' in comp_name:
                if org_name in innodep_today_dict: innodep_today_dict[org_name] += amt
                else: innodep_today_dict[org_name] = amt
                innodep_total_amt += amt

        # 4. ë©”ì¼ ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±
        summary_lines = [f"â­ {d_str} í•™êµ ì§€ëŠ¥í˜• CCTV ë‚©í’ˆ í˜„í™©:"]
        if school_stats:
            for school, info in school_stats.items():
                summary_lines.append(f"- {school} [{info['main_vendor']}]: {info['total_amt']:,}ì›")
        else: summary_lines.append(" 0ê±´")
        
        summary_lines.append(" ") 
        summary_lines.append(f"ğŸ¢ {d_str} ì´ë…¸ë ì‹¤ì :")
        if innodep_today_dict:
            for org, amt in innodep_today_dict.items():
                summary_lines.append(f"- {org}: {amt:,}ì›")
            summary_lines.append(f"** ì´í•©ê³„: {innodep_total_amt:,}ì›")
        else: summary_lines.append(" 0ê±´")

        # 5. ìš©ì—­ ê³„ì•½ ë°ì´í„° HTML ìƒì„±
        servc_html = fetch_and_generate_servc_html(target_dt)

        # 6. GitHub Actionsë¡œ ë°ì´í„° ì „ë‹¬
        if "GITHUB_OUTPUT" in os.environ:
            with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
                f.write(f"collect_date={d_str}\n")
                f.write(f"collect_count={len(final_data)}\n")
                f.write("school_info<<EOF\n")
                for line in summary_lines: f.write(f"{line}<br>\n")
                f.write("EOF\n")
                f.write("servc_info<<EOF\n")
                f.write(f"{servc_html}\n")
                f.write("EOF\n")
    else:
        print(f"â„¹ï¸ {d_str} ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
