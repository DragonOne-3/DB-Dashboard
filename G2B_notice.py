import os
import sys
import json
import datetime
import time
import requests
import pandas as pd
import io
from googleapiclient.discovery import build
from google.oauth2 import service_account
from googleapiclient.http import MediaIoBaseUpload
from concurrent.futures import ThreadPoolExecutor

# ================= í™˜ê²½ ë³€ìˆ˜ =================
SERVICE_KEY = os.environ.get('DATA_GO_KR_API_KEY')
AUTH_JSON_STR = os.environ.get('GOOGLE_AUTH_JSON')

# API URL ë§µ
FILE_MAP = {
    'ê³µì‚¬': 'https://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoCnstwkPPSSrch',
    'ë¬¼í’ˆ': 'https://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoThngPPSSrch',
    'ìš©ì—­': 'https://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoServcPPSSrch'
}

def get_drive_service():
    info = json.loads(AUTH_JSON_STR)
    creds = service_account.Credentials.from_service_account_info(
        info, scopes=['https://www.googleapis.com/auth/drive']
    )
    return build('drive', 'v3', credentials=creds, cache_discovery=False), creds # 3.12 í˜¸í™˜ì„± ìœ„í•´ cache_discovery=False ì¶”ê°€

def fetch_data_chunk(category, url, s_dt, e_dt):
    """ì§€ì •ëœ ê¸°ê°„ ë™ì•ˆì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ (Python 3.12 ìµœì í™”)"""
    all_data = []
    page = 1
    
    # ì„¸ì…˜ ìž¬ì‚¬ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ
    with requests.Session() as session:
        while True:
            params = {
                'serviceKey': SERVICE_KEY, 'pageNo': str(page), 'numOfRows': '999',
                'inqryDiv': '1', 'type': 'json',
                'inqryBgnDt': s_dt + "0000", 'inqryEndDt': e_dt + "2359"
            }
            try:
                res = session.get(url, params=params, timeout=30)
                if res.status_code == 200:
                    res_json = res.json()
                    items = res_json.get('response', {}).get('body', {}).get('items', [])
                    if not items: break
                    all_data.extend(items)
                    
                    # ì „ì²´ ê°œìˆ˜ í™•ì¸ í›„ ë‹¤ìŒ íŽ˜ì´ì§€ ê²°ì •
                    total_count = int(res_json.get('response', {}).get('body', {}).get('totalCount', 0))
                    if len(all_data) >= total_count or len(items) < 999:
                        break
                    page += 1
                else:
                    break
            except Exception as e:
                print(f"âš ï¸ [{category}] {s_dt} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                break
    return pd.DataFrame(all_data)

def process_category(category, url, date_chunks, drive_service, creds):
    """ì¹´í…Œê³ ë¦¬ë³„ ë³‘ë ¬ ìˆ˜ì§‘ ë° ë“œë¼ì´ë¸Œ ì €ìž¥"""
    final_df = pd.DataFrame()
    for s, e in date_chunks:
        print(f"ðŸš€ [{category}] {s} ~ {e} ìˆ˜ì§‘ ì‹œìž‘")
        chunk_df = fetch_data_chunk(category, url, s, e)
        final_df = pd.concat([final_df, chunk_df], ignore_index=True)
    
    if not final_df.empty:
        update_drive(drive_service, creds, category, final_df)

def update_drive(drive_service, creds, cat_name, new_df):
    """êµ¬ê¸€ ë“œë¼ì´ë¸Œ ìµœìƒë‹¨ ì—…ë°ì´íŠ¸ ë¡œì§"""
    file_name = f"ë‚˜ë¼ìž¥í„°_ê³µê³ _{cat_name}.csv"
    
    # íŒŒì¼ ê²€ìƒ‰
    query = f"name='{file_name}' and trashed=false"
    results = drive_service.files().list(q=query, fields='files(id)').execute()
    items = results.get('files', [])

    file_id = items[0]['id'] if items else None
    
    # ê¸°ì¡´ íŒŒì¼ì´ ìžˆìœ¼ë©´ ë³‘í•©
    if file_id:
        download_url = f'https://www.googleapis.com/drive/v3/files/{file_id}?alt=media'
        resp = requests.get(download_url, headers={'Authorization': f'Bearer {creds.token}'})
        if resp.status_code == 200:
            old_df = pd.read_csv(io.BytesIO(resp.content), encoding='utf-8-sig', low_memory=False)
            new_df = pd.concat([old_df, new_df], ignore_index=True)

    # ì¤‘ë³µ ì œê±° (ê³µê³ ë²ˆí˜¸ ê¸°ì¤€) ë° ì—…ë¡œë“œ
    new_df.drop_duplicates(subset=['bidNtceNo'], keep='last', inplace=True)
    csv_bytes = new_df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
    media = MediaIoBaseUpload(io.BytesIO(csv_bytes), mimetype='text/csv', resumable=True)

    if file_id:
        drive_service.files().update(fileId=file_id, media_body=media).execute()
        print(f"âœ… {file_name} ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì´ {len(new_df)}ê±´)")
    else:
        drive_service.files().create(body={'name': file_name}, media_body=media).execute()
        print(f"âœ… {file_name} ì‹ ê·œ ìƒì„± ì™„ë£Œ")

def main():
    if len(sys.argv) < 3: return
    start_str, end_str = sys.argv[1], sys.argv[2]
    
    # 1ê°œì›” ë‹¨ìœ„ë¡œ êµ¬ê°„ ìª¼ê°œê¸°
    start_date = datetime.datetime.strptime(start_str, '%Y%m%d')
    end_date = datetime.datetime.strptime(end_str, '%Y%m%d')
    date_chunks = []
    curr = start_date
    while curr <= end_date:
        next_m = (curr.replace(day=28) + datetime.timedelta(days=4)).replace(day=1)
        chunk_e = min(next_m - datetime.timedelta(days=1), end_date)
        date_chunks.append((curr.strftime('%Y%m%d'), chunk_e.strftime('%Y%m%d')))
        curr = next_m

    drive_service, creds = get_drive_service()
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ 3ê°œ ë™ì‹œ ì‹¤í–‰)
    with ThreadPoolExecutor(max_workers=3) as executor:
        for cat, url in FILE_MAP.items():
            executor.submit(process_category, cat, url, date_chunks, drive_service, creds)

if __name__ == "__main__":
    main()
