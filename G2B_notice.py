import os
import sys
import json
import datetime
import time
import requests
import pandas as pd
import io
import traceback
from googleapiclient.discovery import build
from google.oauth2 import service_account
from googleapiclient.http import MediaIoBaseUpload
from google.auth.transport.requests import Request
from concurrent.futures import ThreadPoolExecutor, as_completed

def log(msg):
    print(msg, flush=True)

# ================= ì„¤ì • =================
SERVICE_KEY = os.environ.get('DATA_GO_KR_API_KEY')
AUTH_JSON_STR = os.environ.get('GOOGLE_AUTH_JSON')

FILE_MAP = {
    'ê³µì‚¬': 'https://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoCnstwkPPSSrch',
    'ë¬¼í’ˆ': 'https://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoThngPPSSrch',
    'ìš©ì—­': 'https://apis.data.go.kr/1230000/ad/BidPublicInfoService/getBidPblancListInfoServcPPSSrch'
}

def get_drive_service():
    info = json.loads(AUTH_JSON_STR)
    creds = service_account.Credentials.from_service_account_info(
        info, scopes=['https://www.googleapis.com/auth/drive']
    )
    # API ìš”ì²­ ì†ë„ ë° ì•ˆì •ì„±ì„ ìœ„í•´ ì „ìš© ì„¸ì…˜ êµ¬ì¶•
    return build('drive', 'v3', credentials=creds, cache_discovery=False), creds

def fetch_data_chunk(category, url, s_dt, e_dt):
    all_data = []
    page = 1
    with requests.Session() as session:
        while True:
            params = {
                'serviceKey': SERVICE_KEY, 'pageNo': str(page), 'numOfRows': '999',
                'inqryDiv': '1', 'type': 'json',
                'inqryBgnDt': s_dt + "0000", 'inqryEndDt': e_dt + "2359"
            }
            log(f"   - [{category}] {s_dt} ~ {e_dt} | {page}p ìš”ì²­")
            
            try:
                res = session.get(url, params=params, timeout=45)
                if res.status_code == 200:
                    res_json = res.json()
                    items = res_json.get('response', {}).get('body', {}).get('items', [])
                    if not items: break
                    all_data.extend(items)
                    total_count = int(res_json.get('response', {}).get('body', {}).get('totalCount', 0))
                    log(f"   - [{category}] ì§„í–‰: {len(all_data)} / {total_count}")
                    if len(all_data) >= total_count or len(items) < 999: break
                    page += 1
                else: break
            except: break
    return pd.DataFrame(all_data)

def update_drive_robust(drive_service, creds, cat_name, new_df):
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ì— ê°•í•œ ì—…ë¡œë“œ ë¡œì§"""
    if new_df.empty: return
    file_name = f"ë‚˜ë¼ìž¥í„°_ê³µê³ _{cat_name}.csv"
    
    try:
        # 1. íŒŒì¼ ì°¾ê¸°
        query = f"name='{file_name}' and trashed=false"
        results = drive_service.files().list(q=query, fields='files(id)').execute()
        items = results.get('files', [])
        file_id = items[0]['id'] if items else None
        
        # 2. ê¸°ì¡´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ë³‘í•©
        if file_id:
            try:
                # í† í° ê°±ì‹ 
                if not creds.valid:
                    creds.refresh(Request())
                
                download_url = f'https://www.googleapis.com/drive/v3/files/{file_id}?alt=media'
                # íƒ€ìž„ì•„ì›ƒ ë„‰ë„‰ížˆ ì„¤ì •
                resp = requests.get(download_url, headers={'Authorization': f'Bearer {creds.token}'}, timeout=60)
                if resp.status_code == 200:
                    # ì¸ì½”ë”© ëŒ€ì‘
                    try:
                        old_df = pd.read_csv(io.BytesIO(resp.content), encoding='utf-8-sig', low_memory=False)
                    except:
                        old_df = pd.read_csv(io.BytesIO(resp.content), encoding='cp949', low_memory=False)
                    new_df = pd.concat([old_df, new_df], ignore_index=True)
            except Exception as e:
                log(f"âš ï¸ [{cat_name}] ê¸°ì¡´ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨(ë³‘í•© ê±´ë„ˆëœ€): {e}")

        # 3. ì¤‘ë³µ ì œê±°
        if 'bidNtceNo' in new_df.columns:
            new_df.drop_duplicates(subset=['bidNtceNo'], keep='last', inplace=True)
            
        # 4. ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ì—…ë¡œë“œ ì¤€ë¹„
        csv_buffer = io.BytesIO()
        new_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
        csv_buffer.seek(0)
        
        media = MediaIoBaseUpload(csv_buffer, mimetype='text/csv', resumable=True)

        # 5. ì—…ë¡œë“œ ìˆ˜í–‰
        if file_id:
            drive_service.files().update(fileId=file_id, media_body=media).execute()
        else:
            drive_service.files().create(body={'name': file_name}, media_body=media).execute()
        log(f"âœ… [{cat_name}] ë“œë¼ì´ë¸Œ ì €ìž¥ ì™„ë£Œ")
        
    except Exception as e:
        log(f"âŒ [{cat_name}] ë“œë¼ì´ë¸Œ ìµœì¢… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        traceback.print_exc()

def process_category(category, url, date_chunks, drive_service, creds):
    # API ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì•½ê°„ì˜ ì‹œì°¨ë¥¼ ë‘ 
    time.sleep({'ê³µì‚¬': 0, 'ë¬¼í’ˆ': 5, 'ìš©ì—­': 10}[category])
    
    for s, e in date_chunks:
        log(f"\nðŸ”„ [{category}] êµ¬ê°„ ì‹œìž‘: {s} ~ {e}")
        chunk_df = fetch_data_chunk(category, url, s, e)
        if not chunk_df.empty:
            update_drive_robust(drive_service, creds, category, chunk_df)
        time.sleep(2)

def main():
    if len(sys.argv) < 3: return
    start_str, end_str = sys.argv[1], sys.argv[2]
    
    start_date = datetime.datetime.strptime(start_str, '%Y%m%d')
    end_date = datetime.datetime.strptime(end_str, '%Y%m%d')
    
    # ë°ì´í„°ê°€ ë§Žìœ¼ë¯€ë¡œ 10ì¼ ë‹¨ìœ„ë¡œ ìª¼ê°¬
    date_chunks = []
    curr = start_date
    while curr <= end_date:
        chunk_e = min(curr + datetime.timedelta(days=9), end_date)
        date_chunks.append((curr.strftime('%Y%m%d'), chunk_e.strftime('%Y%m%d')))
        curr = chunk_e + datetime.timedelta(days=1)

    drive_service, creds = get_drive_service()
    log(f"ðŸ“Š ìˆ˜ì§‘ ì‹œìž‘: {start_str} ~ {end_str}")

    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(process_category, cat, url, date_chunks, drive_service, creds) 
                   for cat, url in FILE_MAP.items()]
        for future in as_completed(futures):
            future.result()
    log("\nðŸ ëª¨ë“  ìˆ˜ì§‘ ìž‘ì—… ì¢…ë£Œ")

if __name__ == "__main__":
    main()
